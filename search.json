[
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing material",
    "section": "",
    "text": "Yaaaay!\nYou have an example of a thing you can do in mgcv? You‚Äôd like to share that? That‚Äôs awesome.\nThe easiest way to do that is to submit a pull request to the site github or suggest a topic via the issue tracker."
  },
  {
    "objectID": "articles/poisson_processes.html",
    "href": "articles/poisson_processes.html",
    "title": "Poisson processes in mgcv",
    "section": "",
    "text": "This article is based on the (excellent!) paper Dovers et al. (2024) (which I reviewed). Go read that paper! This is just a quick note on doing this stuff with a slightly different view on the world.\nThe general idea here is that we want to fit a model to location data, i.e., \\((x, y)\\) locations in space, where the data we have is just the presences of things we saw. Think: making a map of where trees are based on observations of trees. There are specialized ways of doing this but there‚Äôs also a neat trick to do this in any GLM-ish1 framework that interests you.\nOn a side note, I did write this stuff aaaages ago and then neglected to put it anywhere (like a journal), so this also a way to get that stuff out there (thanks to Elliot and co, at least I don‚Äôt have to deal with a journal now!)."
  },
  {
    "objectID": "articles/poisson_processes.html#you-aint-seen-nothin-yet-the-berman-turner-device",
    "href": "articles/poisson_processes.html#you-aint-seen-nothin-yet-the-berman-turner-device",
    "title": "Poisson processes in mgcv",
    "section": "üé∂You ain‚Äôt seen nothin‚Äô yet3üé∂ ‚Äì the Berman-Turner device",
    "text": "üé∂You ain‚Äôt seen nothin‚Äô yet3üé∂ ‚Äì the Berman-Turner device\nWe can re-write the integral above as a quadrature approximation, so\n\\[\\begin{equation*}\n\\Lambda(\\boldsymbol{\\theta})=\\int_{\\Omega}\\lambda(\\mathbf{x},\\boldsymbol{\\theta})\\mathnormal{\\mathrm{d}}\\mathbf{x}\\approx\\sum_{j=1}^{J}w_{j}\\lambda(\\mathbf{s}_{j},\\boldsymbol{\\theta})\n\\end{equation*}\\]\nwith quadrature evaluations (according to some scheme, see below) at \\(\\mathbf{s}_{j}\\) and corresponding weights \\(w_{j}\\). We can then re-write (\\(\\ref{eq:logppm}\\)) as:\n\\[\\begin{equation*}\nl_{\\text{PPM}}\\approx\\sum_{i=1}^{m}\\log_{e}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})-\\sum_{j=1}^{J}w_{j}\\lambda(\\mathbf{s}_{j},\\boldsymbol{\\theta}).\n\\end{equation*}\\]\nThen, concatenating \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{s}_{j}\\) (i.e., combining the observations and quadrature points) into a new \\(\\mathbf{x}_{i}\\), we can re-write this as Baddeley and Turner (2000), we can combine the two terms into one index:\n\\[\\begin{equation}\nl_{\\text{PPM}}\\approx\\sum_{i=1}^{m+J}y_{i}\\log_{e}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})-w_{i}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta}),\n\\label{eq:ppm-approx}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation*}\ny_{i}=\\begin{cases}\n1 & \\text{if }\\mathbf{x}_{i} \\text{ is an observation},\\\\\n0 & \\text{if }\\mathbf{x}_{i} \\text{ is a quadrature point},\n\\end{cases}\n\\end{equation*}\\]\nand extending the \\(w_{i}\\) such that\n\\[\\begin{equation*}\nw_{i}=\\begin{cases}\n1 & \\text{for } i=1,\\ldots,m,\\\\\nw_{j} & \\text{for }i=m+1,\\ldots,m+J \\text{ and }j=1,\\ldots,J.\n\\end{cases}\n\\end{equation*}\\]\nThis looks very close to the form for a Poisson regression with offset \\(w_{i}\\), we have a linear predictor of the form \\(\\eta_{i}=\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta}\\):\n\\[\\begin{align}\nl_{\\text{PO}}(\\boldsymbol{\\beta};\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n};y_{1},\\ldots,y_{n}) &=\\sum_{i=1}^{n}\\left(y_{i}\\eta_{i}-e^{\\eta_{i}}-\\log_{e}\\left(y_{i}!\\right)\\right)\\\\\n&=\\sum_{i=1}^{n}\\left(y_{i}(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})-e^{\\log_{e}w_{i}}e^{\\mathbf{x}_{i}\\boldsymbol{\\theta}}-\\log_{e}\\left(y_{i}!\\right)\\right)\\\\\n&=\\sum_{i=1}^{n}\\left(y_{i}(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})-w_{i}e^{\\mathbf{x}_{i}\\boldsymbol{\\theta}}\\right)\n\\label{eq:poisson-reg}\n\\end{align}\\]\nNote that since \\(y_{i}\\in\\{0,1\\}\\), \\(y_{i}!=1\\Rightarrow\\log_{e}\\left(y_{i}!\\right)=0\\) hence we lose the last term in the second line. So letting \\(\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})=\\exp(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})\\) we have that (\\(\\ref{eq:poisson-reg}\\)) is equivalent to (\\(\\ref{eq:ppm-approx}\\)). Note that sometimes this derivation is via using weights rather than using the offset. This approach is generally referred to as the ‚ÄúBerman-Turner device‚Äù.\nSo to fit the inhomogeneous Poisson process model in (\\(\\ref{eq:pp-lik}\\)), we can fit a Poisson GLM with the following components:\n\nresponse vector \\(\\mathbf{y}=(\\overbrace{1,\\ldots,1}^{m\\text{ times}},\\overbrace{0,\\ldots,0}^{J\\text{ times}})\\),\ndesign matrix \\(\\mathbf{X}\\), where the first \\(m\\) rows are the data locations (and potentially associated covariates), then the following \\(J\\) rows are the locations of the quadrature points,\noffset vector of \\(1\\)s and quadrature weights, \\(\\mathbf{w}=(\\overbrace{1,\\ldots,1}^{m\\text{ times}},w_{1},\\ldots,w_{J})\\).\n\nGenerating an efficient and accurate quadrature scheme for the given problem can potentially be tricky. In one dimension generating a mesh for the quadrature is pretty simple, we can just make an evenly-spaced grid over space (or the range of the covariate). In higher dimensions with uneven data this can be more complicated. Warton and Shepherd (2010) suggest increasing grid complexity until convergence of the maximum likelihood estimate to ensure that the integral is approximated correctly. Simpson et al. (2016) suggest that regular grids are computationally wasteful and suggest the use of triangulation for efficiency."
  },
  {
    "objectID": "articles/poisson_processes.html#footnotes",
    "href": "articles/poisson_processes.html#footnotes",
    "title": "Poisson processes in mgcv",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGeneralized linear model, generalized linear mixed model and of course generalized additive model.‚Ü©Ô∏é\nThat is, where we describe the variation in space using structured random effects that are (multivariate) normal distributed.‚Ü©Ô∏é\nSee https://www.youtube.com/watch?v=4cia_v4vxfE.‚Ü©Ô∏é\nWritten by Finn Lindgren and available on CRAN.‚Ü©Ô∏é\nI‚Äôm using ‚Äúintegration grid‚Äù and ‚Äúmesh‚Äù interchangably here.‚Ü©Ô∏é\nAdapted from here‚Ü©Ô∏é\nA property of being something called a simple point process. Janine Illian once tried to explain this to me but I‚Äôve forgotten now. Sorry Janine!‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yes! You can do that in mgcv",
    "section": "",
    "text": "This is a website about the kinds of things you can do in mgcv that you might not have realised that you can do in mgcv.\nHere are some possible topics:\n\nPoisson process fitting\ncross-validation\nstochastic partial differential equation smoothing\ncapture-recapture (I think?)\nbeing a secret Bayesian and being a more obvious Bayesian\n\nwith bonus material on:\n\nhow to do things not in mgcv (how to import mgcv smoothers into other models/frameworks/software)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why?\nThere‚Äôs lots of information out there about things you can do with mgcv but they‚Äôre all in different places (some of those places are ‚Äúmy hard drive‚Äù), so I thought it would be useful to collect them here for folks to be able to use.\n\n\nWho?\nI‚Äôm Dave Miller. I work at Biomathematics and Statistics Scotland and the UK Centre for Ecology and Hydrology. I first started working with mgcv sometime around 2007."
  },
  {
    "objectID": "articles/gamvar.html",
    "href": "articles/gamvar.html",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "",
    "text": "This document aims to clear up which approaches are appropriate for estimation of variance of the parameters (and dervied quantities from those parameters) of generalized additive models. Three approaches are discussed: analytic, posterior simulation and bootstrap."
  },
  {
    "objectID": "articles/gamvar.html#making-predictions-with-a-gam",
    "href": "articles/gamvar.html#making-predictions-with-a-gam",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Making predictions with a GAM",
    "text": "Making predictions with a GAM\nWe are not usually interested in the variance of the coefficients (\\(\\boldsymbol{\\beta}\\)) themselves, but rather some derived quantity like predictions or summary statistics of predictions. We want to evaluate (\\(\\ref{eq:dsm}\\)) at (new) values of the covariates such that:\n\\[\\begin{equation}\nn_{m}^{*}=\\exp\\left(\\log_{e}A_{m}+\\beta_{0}+f_{\\text{space}}(\\texttt{lat}_{m},\\texttt{lon}_{m})+f_{\\text{depth}}(\\texttt{Depth}_{m})\\right),\\label{eq:dsm-1}\n\\end{equation}\\] In ‚ÄúPractical examples‚Äù below, I‚Äôll show some examples for count data, where the quantity of interest is abundance over some region: sums of predictions over some grid.\nBefore going into variance estimation, we first recap how predictions are formed in GAMs. We have estimated \\(\\hat{\\boldsymbol{\\beta}}\\) and want to make predictions at some new data locations. Prediction is usually explained as ‚Äúplugging-in‚Äù the new covariate values into equation (\\(\\ref{eq:gam}\\)) (or (\\(\\ref{eq:dsm}\\))). This is true, but doesn‚Äôt fully explain what‚Äôs going on. When we build our model, we form a design matrix (\\(\\mathbf{X}\\)), the rows of which correspond to observations and the columns correspond to the basis functions of the smooths (or just the covariates in the case of a GLM). To make a prediction we need to do the same thing again for these (\\(M\\)) new prediction locations, so we form \\(\\mathbf{X}_{p}\\), the (Wood, 2017), which will look something like: \\[\n\\mathbf{X}_{p}=\\left(\\begin{array}{ccccccc}\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},1}) & b_{\\texttt{a},2}(x_{\\texttt{a},1}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},1}) & b_{\\texttt{b},2}(x_{\\texttt{b},1}) & \\dotsc\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},2}) & b_{\\texttt{a},2}(x_{\\texttt{a},2}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},2}) & b_{\\texttt{b},2}(x_{\\texttt{b},2}) & \\dotsc\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & \\dotsc\n\\end{array}\\right),\n\\] where the first column of 1s is for the intercept term and \\(b_{\\texttt{a},j}(x_{\\texttt{a},m})\\) is the evaluation of the \\(j\\)th basis function for covarite \\(\\texttt{a}\\), measured as \\(x_{\\texttt{a},i}\\) for prediction point \\(i\\). Mutliplying \\(\\mathbf{X}_{p}\\) by the estimated model coefficients (\\(\\hat{\\boldsymbol{\\beta}}\\)), we get the linear predictor \\(\\boldsymbol{\\eta}=\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\). We then need to apply the (inverse) link function (\\(g^{-1}\\)) to the linear predictor to get predictions on the correct scale (\\(\\mathbf{y}=g^{-1}\\left(\\boldsymbol{\\eta}\\right)=g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\)). We often want to summarize the resulting predictions, for example by summing them, so we calculate \\(y^{*}=\\sum_{m=1}^{M}y_{m}\\). We could also write this in matrix notation as \\(y^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) where \\(\\mathbf{1}\\) is a row-vector of 1s the same length as \\(\\textbf{y}\\). \\(\\mathbf{X}_{p}\\) is sometimes known as the ‚Äúprojection matrix‚Äù or ‚Äú\\(\\mathbf{L}_{p}\\) matrix‚Äù. It maps the estimated coefficients to the predicted values (on the link scale). Thinking of \\(\\mathbf{X}_{p}\\) as a way to move between the coefficients and derived model quantities (like predictions) is key to understanding how variance can be estimated in the cases below.\nA simple example shows how this works in mgcv:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(2)\ndat &lt;- gamSim(1,n=400,dist=\"normal\",scale=2)\n\nGu & Wahba 4 term additive model\n\n# fit a model\nb &lt;- gam(y~s(x0),data=dat)\n\n# prediction matrix\npred &lt;- data.frame(x0 = seq(0, 1, length.out=100))\n\n# build the Xp matrix\nXp &lt;- predict(b, pred, type=\"lpmatrix\")\n\n# compare \"manual\" generation of predictions to those from predict()\nmanual &lt;- Xp %*% coef(b)\nauto &lt;- predict(b, pred)\n\n# need to convert auto to be a matrix\nidentical(manual, as.matrix(auto))\n\n[1] TRUE"
  },
  {
    "objectID": "articles/gamvar.html#analytic-estimation",
    "href": "articles/gamvar.html#analytic-estimation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic estimation",
    "text": "Analytic estimation\nOnce we have fitted our model, we have not only the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), but also a posterior covariance matrix of those coefficients, \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\). What we‚Äôd really like is the variance of our linear predictor (\\(\\boldsymbol{\\eta}=\\mathbf{X}\\boldsymbol{\\beta}\\)) or some function of the linear predictor (\\(h(\\boldsymbol{\\eta})=h(\\mathbf{X}\\boldsymbol{\\beta}\\))). By the delta method (e.g., (Wasserman, 2004) or (Seber, 1987)), we know that we can calculate the variance of some function of the linear predictor as: \\[\\begin{align}\n\\text{Var}[h(\\boldsymbol{\\eta})] & =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\text{Var}(\\boldsymbol{\\beta})\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)\\\\\n& =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right), \\label{eq:delta-general}\n\\end{align}\\] where the derivative terms are the derivatives of \\(h(\\boldsymbol{\\eta})\\) with respect to the model parameters (\\(\\boldsymbol{\\beta}\\)) evaluated at their estimates. Intuitively we can think of this expression as rotating and rescaling the variance-covariance matrix of the mode l parameters first to the scale of the linear predictor and then into the space of \\(h\\)().\nFollowing on with our density surface example, if we wanted the variance of each prediction, in that case we think of the function \\(h\\) as just the link function \\(h(\\boldsymbol{\\eta})=g(\\boldsymbol{\\eta})\\). If we use a Tweedie for the response and therefore a \\(\\log_{e}\\) link, we have that \\(h(\\hat{\\boldsymbol{\\eta}})=\\text{exp}(\\hat{\\boldsymbol{\\eta}})=\\hat{\\mathbf{n}}\\) (if we start thinking in terms of the estimated model and \\(\\hat{\\mathbf{n}}\\) is a vector of estimated abundances). So we can then calculate: \\[\\begin{align}\n\\text{Var}(\\hat{\\mathbf{n}}) & =\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varn}\n\\end{align}\\] here we rescaling the variance-covariance of \\(\\hat{\\boldsymbol{\\beta}}\\) to be that of the estimated counts. In this case \\(\\text{Var}(\\hat{\\mathbf{n}})\\) would be a vector of variances about the predictions (when calling predict(model, se.fit=TRUE, type=''response'') in mgcv, the $se.fit terms are \\(\\sqrt{\\text{Var}(\\hat{\\mathbf{n}})}\\) (Wood, 2017)) .\nIn our density surface model example, we might really want to know about \\(\\hat{N}=\\sum\\hat{N}_{i}\\) where \\(\\hat{N}_{i}\\) are predicted abundances over some grid and we wish to know the total abundance \\(\\hat{N}\\) and its variance \\(\\text{Var}(\\hat{N})\\):\n\\[\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varNhat}\n\\end{align}\\] where we expand out \\(\\hat{N}=\\sum\\hat{N}_{i}=\\mathbf{A}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) to take the derivative where \\(\\mathbf{A}\\) is a row vector of prediction grid cell areas. Since we usually use a \\(\\log\\)-link, we end up with \\(g^{-1}(x)=\\exp(x)\\), so: \\[\\begin{align*}\n\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}} & =\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left[\\mathbf{A}\\exp\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}\\right)\\right]\\\\\n& =\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right),\n\\end{align*}\\] where \\(\\odot\\) indicates element-wise multiplication (so the \\(i,j\\)th entry in \\(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\) is \\(\\exp\\left(\\left[\\mathbf{X}_{p}\\right]_{.,j}\\hat{\\boldsymbol{\\beta}}\\right)\\left[\\mathbf{X}_{p}\\right]_{ij}\\), where \\(\\left[\\right]_{ij}\\) denotes the \\(i,j\\)th entry of a matrix). So, our final (messy) expression is: \\[\n\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right),\\label{eq:delta-varNhat-1}\n\\end{align}\n\\] This is the method implemented in dsm::dsm.var.gam, dsm::dsm.var.prop and dsm::dsm_varprop. Note that we could instead calculate \\(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\) numerically by finite-differencing."
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation",
    "href": "articles/gamvar.html#posterior-simulation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can also use the fact that fitting a GAM in mgcv is an empirical Bayes procedure, so we have a posterior distribution for \\(\\hat{\\boldsymbol{\\beta}}\\) given the smoothing parameter estimates, \\(\\hat{\\boldsymbol{\\lambda}}\\), (as described above). As an example see Figure Figure¬†1 where posterior samples of \\(\\hat{\\boldsymbol{\\beta}}\\) are used to construct multiple smooths, these smooths then follow the properties of the analytical estimates (based on Bayesian arguments by Marra and Wood (2012)). As can be seen from the plot, the generated grey curves mostly lie within the \\(\\pm2\\) standard error dashed lines.\n\n\n\n\n\n\n\n\nFigure¬†1: Fitted model (black lines) with confidence band (limits in dashed lines) generated from predict(..., se.fit=TRUE) along with 200 samples from the posterior of the model (grey).\n\n\n\n\n\nWe generate the grey curves by simply generating \\(\\boldsymbol{\\beta}_{b}\\sim N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), then multiplying by the prediction matrix (\\(\\mathbf{X}_{p}\\)) to obtain predictions on the linear predictor scale (then applying the link function if necessary). This approach can be particularly handy in the case where we want to calculate the variance of some summary of the linear predictor values. This is particularly useful as the link function is often a non-linear function of the linear predictor, so the approximation in (\\(\\ref{eq:delta-general}\\)) might not be appropriate.\nAs an alternative to (\\(\\ref{eq:delta-varNhat}\\)), the following algorithm can be used:\n\nFor \\(b=1,\\ldots,B\\):\nSimulate from \\(N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), to obtain \\(\\boldsymbol{\\beta}_{b}\\).\nCalculate predicted abundance for this \\(\\boldsymbol{\\beta}_{b}\\), \\(\\hat{N}_{b}^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}_{b}\\right)\\)\nStore \\(\\hat{N}_{b}^{*}\\).\nCalculate the empirical variance or percentiles of the \\(\\hat{N}_{b}\\)s.\n\nIn practice \\(B\\) does not have to be particularly large. (Marra et al., 2012) achieve reasonable results with \\(B=100\\)."
  },
  {
    "objectID": "articles/gamvar.html#bootstrapping",
    "href": "articles/gamvar.html#bootstrapping",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nA popular reply to the question of how to calculate uncertainty for complex models is ‚Äúdo a bootstrap‚Äù. What is meant by this is usually that one should resample the data with replacement, refitting models each time and calculating some summary statistic. This seems appealing but has been shown (Carlin and Gelfand, 1991; Laird and Louis, 1987) that the use of so-called ‚ÄúnaÔøΩve‚Äù bootstraps leads to underestimation of uncertainty. The issue stems from the fact that GAM terms are essentially structured random effects, which have priors on them. When data is resampled, the prior structure is ignored so the prior uncertainty is collapsed leaving only the sampling variation in the bootstrap resamples. (Bravington et al., 2018) show a simple simulated example of this happening. Simply put, the bootstrap makes the assumption that all possible distinct values of the covariates have been observed (Rubin, 1981).\n\n\nRasmus BÔøΩÔøΩth gives a good explanation of a non-parametric bootstrap from a Bayesian point of view here: [http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/].\nIt‚Äôs not impossible to concoct some kind of special bootstrap that could deal with this situation, what is sure is that this would require some careful thinking in each situation. The previous two methods just work."
  },
  {
    "objectID": "articles/gamvar.html#analytic-method",
    "href": "articles/gamvar.html#analytic-method",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic method",
    "text": "Analytic method\nTo make the analytic variance calculation given in (\\(\\ref{eq:delta-varNhat}\\)), we need the ingredients for that: \\(\\hat{\\boldsymbol{\\beta}}\\) (the estimated coefficients, \\(\\mathbf{X}_{p}\\) (the prediction matrix) and \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) (posterior variance-covariance matrix for \\(\\hat{\\boldsymbol{\\beta}}\\)). These are obtained as follows:\n\n# the coefficients\nbetas &lt;- m$coef\n# prediction matrix\nXp &lt;- predict(m, predgrid, type=\"lpmatrix\")\n# posterior variance-covariance for the betas\nVbeta &lt;- vcov(m, unconditional=TRUE)\n\nWe can then simply write-out the mathematics:\n\n# row vector of areas\nareas &lt;- matrix(predgrid$off.set, nrow=1)\n\n# calculate the \"bread\" (either side of our matrix \"sandwich\")\n# calculate this in 2 bits...\nexpxb &lt;- diag(exp(Xp%*%betas)[,1], ncol=nrow(Xp), nrow=nrow(Xp))\nbread &lt;- expxb %*% Xp\n\n# get the variance without the offset\nV_analytic &lt;- bread %*% Vbeta %*% t(bread)\n\n# multiply up by the offset\nV_analytic &lt;- areas %*% V_analytic %*% t(areas)\n\n# standard error\nsqrt(V_analytic)\n\n        [,1]\n[1,] 344.953\n\n# CV\nsqrt(V_analytic)/Nhat\n\n          [,1]\n[1,] 0.1316786"
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation-1",
    "href": "articles/gamvar.html#posterior-simulation-1",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can get to the same result by doing posterior simulation, using the variables betas, Xp and Vbeta from the above:\n\nset.seed(211)\n\n# number of samples\nnsamp &lt;- 200\n\n# storage for samples\nres &lt;- rep(0, nsamp)\n\nfor (i in 1:nsamp){\n  # sample from the posterior of the model parameters\n  br &lt;- matrix(rmvn(1, betas, Vbeta))\n  # make a prediction on the link scale\n  pr &lt;- Xp %*% br\n  # offset and transform, storing in res\n  res[i] &lt;- sum(predgrid$off.set*exp(pr))\n}\n\n# can do this quickly in 2 lines as\n# br &lt;- rmvn(nsamp, betas, Vbeta)\n# res &lt;- colSums(predgrid$off.set * exp(Xp %*% t(br)))\n\n# calculate the standard error\nsqrt(var(res))\n\n[1] 393.0305\n\n# CV\nsqrt(var(res))/Nhat\n\n[1] 0.1500312\n\n\nNote that here we see that the CV (and standard error) is a little larger than for the delta method. In part because of the link function making extreme values more extreme (but only in one direction). We could get around this by implementing some kind of importance (or Metropolis-Hastings) sampler, ensuring that more values are sampled near ‚Äúrealistic‚Äù values or simply by increasing nsamp. Since we are sampling from a multivariate normal, the procedure is relatively fast and we can afford to make nsamp relatively large. The likely bottleneck in the code is the matrix multiplication when the two line method is used.\n\n\n\n\n\n\n\n\nFigure¬†3: Left: histogram of posterior samples of abundance for the sperm whale data. Right: posterior samples from the depth smooth."
  },
  {
    "objectID": "articles/gamvar.html#discussion",
    "href": "articles/gamvar.html#discussion",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Discussion",
    "text": "Discussion\nAbove I‚Äôve described three methods for estimating variance from GAMs, specifically with examples from spatial modelling of distance sampling data. It has been clear for some time that nonparametric bootstrap-based methods are not appropriate for models with random effects, but the alternatives have not been laid-out, especially for spatial data. Hopefully the examples here show how internal functions in dsm work, and how they can be implemented for those who don‚Äôt use that package.\nFor those dealing with DSMs, the above doesn‚Äôt directly address the issue of variance that comes from the detection function. Fortunately, including this source of uncertainty comes without too much additional effort. If we use the methods described in (Bravington et al., 2018), we can obtain a \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) that includes detection function uncertainty. We can then use that \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) in the procedures outlined above."
  },
  {
    "objectID": "articles/gamvar.html#acknowledgements",
    "href": "articles/gamvar.html#acknowledgements",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was originally funded by OPNAV N45 and the SURTASS LFA Settlement Agreement, and being managed by the U.S. Navy‚Äôs Living Marine Resources program under Contract No.¬†N39430-17-C-1982."
  },
  {
    "objectID": "articles/prediction_intervals.html",
    "href": "articles/prediction_intervals.html",
    "title": "Prediction intervals",
    "section": "",
    "text": "We can make two kinds of predictions from our model [Faraway (2002); Section 3.5]:\nIn both cases the mean effect will be the same (so we can simply use the output from predict(model) but there are differences in how to calculate the uncertainty. In the latter case we need to include the additional variance from the error term in the model (usually denoted \\(\\epsilon\\)). We call this second case a prediction interval.\nThinking about the normal response case (ignoring link functions etc for now), if we can write down the linear predictor of our model as \\(\\boldsymbol{X}\\boldsymbol{\\beta}\\), we want \\(\\text{Var}(\\boldsymbol{X}\\boldsymbol{\\beta})\\) in case 1. above. For case 2. we have the prediction as \\(\\boldsymbol{X}\\boldsymbol{\\beta} + \\epsilon\\). \\(\\mathbf{E}(\\epsilon) = 0\\), so we ignore this for the mean effect, but we need to take \\(\\epsilon\\) into account.\nMore generally, we want to take into account uncertainty from the various hyperparameters of the response distribution, like scale, shape etc.\nIt‚Äôs simplest to show this via posterior simulation, so that‚Äôs what is used here."
  },
  {
    "objectID": "articles/prediction_intervals.html#example-1---normal-response",
    "href": "articles/prediction_intervals.html#example-1---normal-response",
    "title": "Prediction intervals",
    "section": "Example 1 - normal response",
    "text": "Example 1 - normal response\nFor the first example, we‚Äôll simulate some normally-distributed data using the gamSim function in mgcv, but only use one predictor, for simplicity.\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(8)\ndat &lt;- gamSim()\n\nGu & Wahba 4 term additive model\n\n# just want one predictor here to make y just f2 + noise\ndat$y &lt;- dat$f2 + rnorm(400, 0, 1)\n\nWe can then fit an extremely boring model to this data:\n\n## fit smooth model to x, y data...\nb &lt;- gam(y~s(x2,k=20), method=\"REML\", data=dat)\n\nNow to the central bit of the problem\n\n## simulate replicate beta vectors from posterior...\nn.rep &lt;- 10000\nbr &lt;- rmvn(n.rep, coef(b), vcov(b))\n\n## turn these into replicate smooths\nxp &lt;- seq(0, 1, length.out=200)\n\n# this is the \"Lp matrix\" which maps the coefficients to the predictors\n# you can think of it like the design matrix but for the predictions\nXp &lt;- predict(b, newdata=data.frame(x2=xp), type=\"lpmatrix\")\n\n# note that we should apply the link function here if we have one\n# this is now a matrix with n.rep replicate smooths over length(xp) locations\nfv &lt;- Xp%*%t(br)\n\n# now simulate from normal deviates with mean as in fv\n# and estimated scale...\nyr &lt;- matrix(rnorm(length(fv), fv, b$scale), nrow(fv), ncol(fv))\n\nnow make some plots\n\n# plot the replicates in yr\nplot(rep(xp, n.rep), yr, pch=\".\")\n# and the data we used to fit the model\npoints(dat$x2, dat$y, pch=19, cex=.5)\n\n# compute 95% prediction interval\n# since yr is a matrix where each row is a data location along x2 and\n# each column is a replicate we want the quantiles over the rows, summarizing\n# the replicates each time\nPI &lt;- apply(yr, 1, quantile, prob=c(.025,0.975))\n# the result has the 2.5% bound in the first row and the 97.5% bound\n# in the second\n\n# we can then plot those two bounds\nlines(xp, PI[1,], col=2, lwd=2)\nlines(xp, PI[2,], col=2, lwd=2)\n\n# and optionally add the confidence interval for comparison...\npred &lt;- predict(b, newdata=data.frame(x2=xp), se=TRUE)\nlines(xp, pred$fit, col=3, lwd=2)\nu.ci &lt;- pred$fit + 2*pred$se.fit\nl.ci &lt;- pred$fit - 2*pred$se.fit\nlines(xp, u.ci, col=3, lwd=2)\nlines(xp, l.ci, col=3, lwd=2)"
  }
]