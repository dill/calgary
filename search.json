[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why?\nThere’s lots of information out there about things you can do with mgcv but they’re all in different places (some of those places are “my hard drive”), so I thought it would be useful to collect them here for folks to be able to use.\n\n\nWho?\nI’m Dave Miller. I work at Biomathematics and Statistics Scotland and the UK Centre for Ecology and Hydrology. I first started working with mgcv sometime around 2007."
  },
  {
    "objectID": "articles/prediction_intervals.html",
    "href": "articles/prediction_intervals.html",
    "title": "Prediction intervals",
    "section": "",
    "text": "Based on https://stat.ethz.ch/pipermail/r-help/2011-April/275632.html\n\n## Prediction interval example for a GAM\n# based on https://stat.ethz.ch/pipermail/r-help/2011-April/275632.html\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(8)\ndat &lt;- gamSim()\n\nGu & Wahba 4 term additive model\n\n# just want one predictor here to make y just f2 + noise\ndat$y &lt;- dat$f2 + rnorm(400, 0, 1)\n\n## fit smooth model to x, y data...\nb &lt;- gam(y~s(x2,k=20), method=\"REML\", data=dat)\n\n## simulate replicate beta vectors from posterior...\nn.rep &lt;- 10000\nbr &lt;- rmvn(n.rep, coef(b), vcov(b))\n\n## turn these into replicate smooths\nxp &lt;- seq(0, 1, length.out=200)\n# this is the \"Lp matrix\" which maps the coefficients to the predictors\n# you can think of it like the design matrix but for the predictions\nXp &lt;- predict(b, newdata=data.frame(x2=xp), type=\"lpmatrix\")\n# note that we should apply the link function here if we have one\n# this is now a matrix with n.rep replicate smooths over length(xp) locations\nfv &lt;- Xp%*%t(br)\n\n# now simulate from normal deviates with mean as in fv\n# and estimated scale...\n# (can replace rnorm at this point with the distribution of the data we're\n#  using in a given example)\nyr &lt;- matrix(rnorm(length(fv), fv, b$scale), nrow(fv), ncol(fv))\n\n\n# now make some plots\n\n# plot the replicates in yr\nplot(rep(xp, n.rep), yr, pch=\".\")\n# and the data we used to fit the model\npoints(dat$x2, dat$y, pch=19, cex=.5)\n\n# compute 95% prediction interval\n# since yr is a matrix where each row is a data location along x2 and\n# each column is a replicate we want the quantiles over the rows, summarizing\n# the replicates each time\nPI &lt;- apply(yr, 1, quantile, prob=c(.025,0.975))\n# the result has the 2.5% bound in the first row and the 97.5% bound\n# in the second\n\n# we can then plot those two bounds\nlines(xp, PI[1,], col=2, lwd=2)\nlines(xp, PI[2,], col=2, lwd=2)\n\n# and optionally add the confidence interval for comparison...\npred &lt;- predict(b, newdata=data.frame(x2=xp), se=TRUE)\nlines(xp, pred$fit, col=3, lwd=2)\nu.ci &lt;- pred$fit + 2*pred$se.fit\nl.ci &lt;- pred$fit - 2*pred$se.fit\nlines(xp, u.ci, col=3, lwd=2)\nlines(xp, l.ci, col=3, lwd=2)"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing material",
    "section": "",
    "text": "Yaaaay!\nYou have an example of a thing you can do in mgcv? You’d like to share that? That’s awesome.\nThe easiest way to do that is to submit a pull request to the site github or suggest a topic via the issue tracker."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yes! You can do that in mgcv",
    "section": "",
    "text": "This is a website about the kinds of things you can do in mgcv that you might not have realised that you can do in mgcv.\nHere are some possible topics:\n\nPoisson process fitting\ncross-validation\nstochastic partial differential equation smoothing\ncapture-recapture (I think?)\nbeing a secret Bayesian and being a more obvious Bayesian\n\nwith bonus material on:\n\nhow to do things not in mgcv (how to import mgcv smoothers into other models/frameworks/software)"
  },
  {
    "objectID": "articles/gamvar.html",
    "href": "articles/gamvar.html",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "",
    "text": "This document aims to clear up which approaches are appropriate for estimation of variance of the parameters (and dervied quantities from those parameters) of generalized additive models. Three approaches are discussed: analytic, posterior simulation and bootstrap."
  },
  {
    "objectID": "articles/gamvar.html#making-predictions-with-a-gam",
    "href": "articles/gamvar.html#making-predictions-with-a-gam",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Making predictions with a GAM",
    "text": "Making predictions with a GAM\nWe are not usually interested in the variance of the coefficients (\\(\\boldsymbol{\\beta}\\)) themselves, but rather some derived quantity like predictions or summary statistics of predictions. We want to evaluate (\\(\\ref{eq:dsm}\\)) at (new) values of the covariates such that:\n\\[\\begin{equation}\nn_{m}^{*}=\\exp\\left(\\log_{e}A_{m}+\\beta_{0}+f_{\\text{space}}(\\texttt{lat}_{m},\\texttt{lon}_{m})+f_{\\text{depth}}(\\texttt{Depth}_{m})\\right),\\label{eq:dsm-1}\n\\end{equation}\\] In “Practical examples” below, I’ll show some examples for count data, where the quantity of interest is abundance over some region: sums of predictions over some grid.\nBefore going into variance estimation, we first recap how predictions are formed in GAMs. We have estimated \\(\\hat{\\boldsymbol{\\beta}}\\) and want to make predictions at some new data locations. Prediction is usually explained as “plugging-in” the new covariate values into equation (\\(\\ref{eq:gam}\\)) (or (\\(\\ref{eq:dsm}\\))). This is true, but doesn’t fully explain what’s going on. When we build our model, we form a design matrix (\\(\\mathbf{X}\\)), the rows of which correspond to observations and the columns correspond to the basis functions of the smooths (or just the covariates in the case of a GLM). To make a prediction we need to do the same thing again for these (\\(M\\)) new prediction locations, so we form \\(\\mathbf{X}_{p}\\), the (Wood, 2017), which will look something like: \\[\n\\mathbf{X}_{p}=\\left(\\begin{array}{ccccccc}\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},1}) & b_{\\texttt{a},2}(x_{\\texttt{a},1}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},1}) & b_{\\texttt{b},2}(x_{\\texttt{b},1}) & \\dotsc\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},2}) & b_{\\texttt{a},2}(x_{\\texttt{a},2}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},2}) & b_{\\texttt{b},2}(x_{\\texttt{b},2}) & \\dotsc\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & \\dotsc\n\\end{array}\\right),\n\\] where the first column of 1s is for the intercept term and \\(b_{\\texttt{a},j}(x_{\\texttt{a},m})\\) is the evaluation of the \\(j\\)th basis function for covarite \\(\\texttt{a}\\), measured as \\(x_{\\texttt{a},i}\\) for prediction point \\(i\\). Mutliplying \\(\\mathbf{X}_{p}\\) by the estimated model coefficients (\\(\\hat{\\boldsymbol{\\beta}}\\)), we get the linear predictor \\(\\boldsymbol{\\eta}=\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\). We then need to apply the (inverse) link function (\\(g^{-1}\\)) to the linear predictor to get predictions on the correct scale (\\(\\mathbf{y}=g^{-1}\\left(\\boldsymbol{\\eta}\\right)=g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\)). We often want to summarize the resulting predictions, for example by summing them, so we calculate \\(y^{*}=\\sum_{m=1}^{M}y_{m}\\). We could also write this in matrix notation as \\(y^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) where \\(\\mathbf{1}\\) is a row-vector of 1s the same length as \\(\\textbf{y}\\). \\(\\mathbf{X}_{p}\\) is sometimes known as the “projection matrix” or “\\(\\mathbf{L}_{p}\\) matrix”. It maps the estimated coefficients to the predicted values (on the link scale). Thinking of \\(\\mathbf{X}_{p}\\) as a way to move between the coefficients and derived model quantities (like predictions) is key to understanding how variance can be estimated in the cases below.\nA simple example shows how this works in mgcv:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-1. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(2)\ndat &lt;- gamSim(1,n=400,dist=\"normal\",scale=2)\n\nGu & Wahba 4 term additive model\n\n# fit a model\nb &lt;- gam(y~s(x0),data=dat)\n\n# prediction matrix\npred &lt;- data.frame(x0 = seq(0, 1, length.out=100))\n\n# build the Xp matrix\nXp &lt;- predict(b, pred, type=\"lpmatrix\")\n\n# compare \"manual\" generation of predictions to those from predict()\nmanual &lt;- Xp %*% coef(b)\nauto &lt;- predict(b, pred)\n\n# need to convert auto to be a matrix\nidentical(manual, as.matrix(auto))\n\n[1] TRUE"
  },
  {
    "objectID": "articles/gamvar.html#analytic-estimation",
    "href": "articles/gamvar.html#analytic-estimation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic estimation",
    "text": "Analytic estimation\nOnce we have fitted our model, we have not only the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), but also a posterior covariance matrix of those coefficients, \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\). What we’d really like is the variance of our linear predictor (\\(\\boldsymbol{\\eta}=\\mathbf{X}\\boldsymbol{\\beta}\\)) or some function of the linear predictor (\\(h(\\boldsymbol{\\eta})=h(\\mathbf{X}\\boldsymbol{\\beta}\\))). By the delta method (e.g., (Wasserman, 2004) or (Seber, 1987)), we know that we can calculate the variance of some function of the linear predictor as: \\[\\begin{align}\n\\text{Var}[h(\\boldsymbol{\\eta})] & =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\text{Var}(\\boldsymbol{\\beta})\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)\\\\\n& =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right), \\label{eq:delta-general}\n\\end{align}\\] where the derivative terms are the derivatives of \\(h(\\boldsymbol{\\eta})\\) with respect to the model parameters (\\(\\boldsymbol{\\beta}\\)) evaluated at their estimates. Intuitively we can think of this expression as rotating and rescaling the variance-covariance matrix of the mode l parameters first to the scale of the linear predictor and then into the space of \\(h\\)().\nFollowing on with our density surface example, if we wanted the variance of each prediction, in that case we think of the function \\(h\\) as just the link function \\(h(\\boldsymbol{\\eta})=g(\\boldsymbol{\\eta})\\). If we use a Tweedie for the response and therefore a \\(\\log_{e}\\) link, we have that \\(h(\\hat{\\boldsymbol{\\eta}})=\\text{exp}(\\hat{\\boldsymbol{\\eta}})=\\hat{\\mathbf{n}}\\) (if we start thinking in terms of the estimated model and \\(\\hat{\\mathbf{n}}\\) is a vector of estimated abundances). So we can then calculate: \\[\\begin{align}\n\\text{Var}(\\hat{\\mathbf{n}}) & =\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varn}\n\\end{align}\\] here we rescaling the variance-covariance of \\(\\hat{\\boldsymbol{\\beta}}\\) to be that of the estimated counts. In this case \\(\\text{Var}(\\hat{\\mathbf{n}})\\) would be a vector of variances about the predictions (when calling predict(model, se.fit=TRUE, type=''response'') in mgcv, the $se.fit terms are \\(\\sqrt{\\text{Var}(\\hat{\\mathbf{n}})}\\) (Wood, 2017)) .\nIn our density surface model example, we might really want to know about \\(\\hat{N}=\\sum\\hat{N}_{i}\\) where \\(\\hat{N}_{i}\\) are predicted abundances over some grid and we wish to know the total abundance \\(\\hat{N}\\) and its variance \\(\\text{Var}(\\hat{N})\\):\n\\[\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varNhat}\n\\end{align}\\] where we expand out \\(\\hat{N}=\\sum\\hat{N}_{i}=\\mathbf{A}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) to take the derivative where \\(\\mathbf{A}\\) is a row vector of prediction grid cell areas. Since we usually use a \\(\\log\\)-link, we end up with \\(g^{-1}(x)=\\exp(x)\\), so: \\[\\begin{align*}\n\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}} & =\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left[\\mathbf{A}\\exp\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}\\right)\\right]\\\\\n& =\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right),\n\\end{align*}\\] where \\(\\odot\\) indicates element-wise multiplication (so the \\(i,j\\)th entry in \\(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\) is \\(\\exp\\left(\\left[\\mathbf{X}_{p}\\right]_{.,j}\\hat{\\boldsymbol{\\beta}}\\right)\\left[\\mathbf{X}_{p}\\right]_{ij}\\), where \\(\\left[\\right]_{ij}\\) denotes the \\(i,j\\)th entry of a matrix). So, our final (messy) expression is: \\[\n\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right),\\label{eq:delta-varNhat-1}\n\\end{align}\n\\] This is the method implemented in dsm::dsm.var.gam, dsm::dsm.var.prop and dsm::dsm_varprop. Note that we could instead calculate \\(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\) numerically by finite-differencing."
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation",
    "href": "articles/gamvar.html#posterior-simulation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can also use the fact that fitting a GAM in mgcv is an empirical Bayes procedure, so we have a posterior distribution for \\(\\hat{\\boldsymbol{\\beta}}\\) given the smoothing parameter estimates, \\(\\hat{\\boldsymbol{\\lambda}}\\), (as described above). As an example see Figure Figure 1 where posterior samples of \\(\\hat{\\boldsymbol{\\beta}}\\) are used to construct multiple smooths, these smooths then follow the properties of the analytical estimates (based on Bayesian arguments by Marra and Wood (2012)). As can be seen from the plot, the generated grey curves mostly lie within the \\(\\pm2\\) standard error dashed lines.\n\n\n\n\n\nFigure 1: Fitted model (black lines) with confidence band (limits in dashed lines) generated from predict(..., se.fit=TRUE) along with 200 samples from the posterior of the model (grey).\n\n\n\n\nWe generate the grey curves by simply generating \\(\\boldsymbol{\\beta}_{b}\\sim N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), then multiplying by the prediction matrix (\\(\\mathbf{X}_{p}\\)) to obtain predictions on the linear predictor scale (then applying the link function if necessary). This approach can be particularly handy in the case where we want to calculate the variance of some summary of the linear predictor values. This is particularly useful as the link function is often a non-linear function of the linear predictor, so the approximation in (\\(\\ref{eq:delta-general}\\)) might not be appropriate.\nAs an alternative to (\\(\\ref{eq:delta-varNhat}\\)), the following algorithm can be used:\n\nFor \\(b=1,\\ldots,B\\):\nSimulate from \\(N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), to obtain \\(\\boldsymbol{\\beta}_{b}\\).\nCalculate predicted abundance for this \\(\\boldsymbol{\\beta}_{b}\\), \\(\\hat{N}_{b}^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}_{b}\\right)\\)\nStore \\(\\hat{N}_{b}^{*}\\).\nCalculate the empirical variance or percentiles of the \\(\\hat{N}_{b}\\)s.\n\nIn practice \\(B\\) does not have to be particularly large. (Marra et al., 2012) achieve reasonable results with \\(B=100\\)."
  },
  {
    "objectID": "articles/gamvar.html#bootstrapping",
    "href": "articles/gamvar.html#bootstrapping",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nA popular reply to the question of how to calculate uncertainty for complex models is “do a bootstrap”. What is meant by this is usually that one should resample the data with replacement, refitting models each time and calculating some summary statistic. This seems appealing but has been shown (Carlin and Gelfand, 1991; Laird and Louis, 1987) that the use of so-called “na�ve” bootstraps leads to underestimation of uncertainty. The issue stems from the fact that GAM terms are essentially structured random effects, which have priors on them. When data is resampled, the prior structure is ignored so the prior uncertainty is collapsed leaving only the sampling variation in the bootstrap resamples. (Bravington et al., 2018) show a simple simulated example of this happening. Simply put, the bootstrap makes the assumption that all possible distinct values of the covariates have been observed (Rubin, 1981).\n\n\nRasmus B��th gives a good explanation of a non-parametric bootstrap from a Bayesian point of view here: [http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/].\nIt’s not impossible to concoct some kind of special bootstrap that could deal with this situation, what is sure is that this would require some careful thinking in each situation. The previous two methods just work."
  },
  {
    "objectID": "articles/gamvar.html#analytic-method",
    "href": "articles/gamvar.html#analytic-method",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic method",
    "text": "Analytic method\nTo make the analytic variance calculation given in (\\(\\ref{eq:delta-varNhat}\\)), we need the ingredients for that: \\(\\hat{\\boldsymbol{\\beta}}\\) (the estimated coefficients, \\(\\mathbf{X}_{p}\\) (the prediction matrix) and \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) (posterior variance-covariance matrix for \\(\\hat{\\boldsymbol{\\beta}}\\)). These are obtained as follows:\n\n# the coefficients\nbetas &lt;- m$coef\n# prediction matrix\nXp &lt;- predict(m, predgrid, type=\"lpmatrix\")\n# posterior variance-covariance for the betas\nVbeta &lt;- vcov(m, unconditional=TRUE)\n\nWe can then simply write-out the mathematics:\n\n# row vector of areas\nareas &lt;- matrix(predgrid$off.set, nrow=1)\n\n# calculate the \"bread\" (either side of our matrix \"sandwich\")\n# calculate this in 2 bits...\nexpxb &lt;- diag(exp(Xp%*%betas)[,1], ncol=nrow(Xp), nrow=nrow(Xp))\nbread &lt;- expxb %*% Xp\n\n# get the variance without the offset\nV_analytic &lt;- bread %*% Vbeta %*% t(bread)\n\n# multiply up by the offset\nV_analytic &lt;- areas %*% V_analytic %*% t(areas)\n\n# standard error\nsqrt(V_analytic)\n\n        [,1]\n[1,] 344.953\n\n# CV\nsqrt(V_analytic)/Nhat\n\n          [,1]\n[1,] 0.1316786"
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation-1",
    "href": "articles/gamvar.html#posterior-simulation-1",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can get to the same result by doing posterior simulation, using the variables betas, Xp and Vbeta from the above:\n\nset.seed(211)\n\n# number of samples\nnsamp &lt;- 200\n\n# storage for samples\nres &lt;- rep(0, nsamp)\n\nfor (i in 1:nsamp){\n  # sample from the posterior of the model parameters\n  br &lt;- matrix(rmvn(1, betas, Vbeta))\n  # make a prediction on the link scale\n  pr &lt;- Xp %*% br\n  # offset and transform, storing in res\n  res[i] &lt;- sum(predgrid$off.set*exp(pr))\n}\n\n# can do this quickly in 2 lines as\n# br &lt;- rmvn(nsamp, betas, Vbeta)\n# res &lt;- colSums(predgrid$off.set * exp(Xp %*% t(br)))\n\n# calculate the standard error\nsqrt(var(res))\n\n[1] 393.0305\n\n# CV\nsqrt(var(res))/Nhat\n\n[1] 0.1500312\n\n\nNote that here we see that the CV (and standard error) is a little larger than for the delta method. In part because of the link function making extreme values more extreme (but only in one direction). We could get around this by implementing some kind of importance (or Metropolis-Hastings) sampler, ensuring that more values are sampled near “realistic” values or simply by increasing nsamp. Since we are sampling from a multivariate normal, the procedure is relatively fast and we can afford to make nsamp relatively large. The likely bottleneck in the code is the matrix multiplication when the two line method is used.\n\n\n\n\n\nFigure 3: Left: histogram of posterior samples of abundance for the sperm whale data. Right: posterior samples from the depth smooth."
  },
  {
    "objectID": "articles/gamvar.html#discussion",
    "href": "articles/gamvar.html#discussion",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Discussion",
    "text": "Discussion\nAbove I’ve described three methods for estimating variance from GAMs, specifically with examples from spatial modelling of distance sampling data. It has been clear for some time that nonparametric bootstrap-based methods are not appropriate for models with random effects, but the alternatives have not been laid-out, especially for spatial data. Hopefully the examples here show how internal functions in dsm work, and how they can be implemented for those who don’t use that package.\nFor those dealing with DSMs, the above doesn’t directly address the issue of variance that comes from the detection function. Fortunately, including this source of uncertainty comes without too much additional effort. If we use the methods described in (Bravington et al., 2018), we can obtain a \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) that includes detection function uncertainty. We can then use that \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) in the procedures outlined above."
  },
  {
    "objectID": "articles/gamvar.html#acknowledgements",
    "href": "articles/gamvar.html#acknowledgements",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was originally funded by OPNAV N45 and the SURTASS LFA Settlement Agreement, and being managed by the U.S. Navy’s Living Marine Resources program under Contract No. N39430-17-C-1982."
  }
]