[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why?\nThere‚Äôs lots of information out there about things you can do with mgcv but they‚Äôre all in different places (some of those places are ‚Äúmy hard drive‚Äù), so I thought it would be useful to collect them here for folks to be able to use.\n\n\nWho?\nI‚Äôm Dave Miller. I work at Biomathematics and Statistics Scotland and the UK Centre for Ecology and Hydrology. I first started working with mgcv sometime around 2007."
  },
  {
    "objectID": "articles/poisson_processes.html",
    "href": "articles/poisson_processes.html",
    "title": "Poisson processes in mgcv",
    "section": "",
    "text": "This article is based on the (excellent!) paper Dovers et al. (2024) (which I reviewed). Go read that paper! This is just a quick note on doing this stuff with a slightly different view on the world.\nThe general idea here is that we want to fit a model to location data, i.e., \\((x, y)\\) locations in space, where the data we have is just the presences of things we saw. Think: making a map of where trees are based on observations of trees. There are specialized ways of doing this but there‚Äôs also a neat trick to do this in any GLM-ish1 framework that interests you.\nOn a side note, I did write this stuff aaaages ago and then neglected to put it anywhere (like a journal), so this also a way to get that stuff out there (thanks to Elliot and co, at least I don‚Äôt have to deal with a journal now!)."
  },
  {
    "objectID": "articles/poisson_processes.html#you-aint-seen-nothin-yet-the-berman-turner-device",
    "href": "articles/poisson_processes.html#you-aint-seen-nothin-yet-the-berman-turner-device",
    "title": "Poisson processes in mgcv",
    "section": "üé∂You ain‚Äôt seen nothin‚Äô yet3üé∂ ‚Äì the Berman-Turner device",
    "text": "üé∂You ain‚Äôt seen nothin‚Äô yet3üé∂ ‚Äì the Berman-Turner device\nWe can re-write the integral above as a quadrature approximation, so\n\\[\\begin{equation*}\n\\Lambda(\\boldsymbol{\\theta})=\\int_{\\Omega}\\lambda(\\mathbf{x},\\boldsymbol{\\theta})\\mathnormal{\\mathrm{d}}\\mathbf{x}\\approx\\sum_{j=1}^{J}w_{j}\\lambda(\\mathbf{s}_{j},\\boldsymbol{\\theta})\n\\end{equation*}\\]\nwith quadrature evaluations (according to some scheme, see below) at \\(\\mathbf{s}_{j}\\) and corresponding weights \\(w_{j}\\). We can then re-write (\\(\\ref{eq:logppm}\\)) as:\n\\[\\begin{equation*}\nl_{\\text{PPM}}\\approx\\sum_{i=1}^{m}\\log_{e}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})-\\sum_{j=1}^{J}w_{j}\\lambda(\\mathbf{s}_{j},\\boldsymbol{\\theta}).\n\\end{equation*}\\]\nThen, concatenating \\(\\mathbf{x}_{i}\\) and \\(\\mathbf{s}_{j}\\) (i.e., combining the observations and quadrature points) into a new \\(\\mathbf{x}_{i}\\), we can re-write this as Baddeley and Turner (2000), we can combine the two terms into one index:\n\\[\\begin{equation}\nl_{\\text{PPM}}\\approx\\sum_{i=1}^{m+J}y_{i}\\log_{e}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})-w_{i}\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta}),\n\\label{eq:ppm-approx}\n\\end{equation}\\]\nwhere\n\\[\\begin{equation*}\ny_{i}=\\begin{cases}\n1 & \\text{if }\\mathbf{x}_{i} \\text{ is an observation},\\\\\n0 & \\text{if }\\mathbf{x}_{i} \\text{ is a quadrature point},\n\\end{cases}\n\\end{equation*}\\]\nand extending the \\(w_{i}\\) such that\n\\[\\begin{equation*}\nw_{i}=\\begin{cases}\n1 & \\text{for } i=1,\\ldots,m,\\\\\nw_{j} & \\text{for }i=m+1,\\ldots,m+J \\text{ and }j=1,\\ldots,J.\n\\end{cases}\n\\end{equation*}\\]\nThis looks very close to the form for a Poisson regression with offset \\(w_{i}\\), we have a linear predictor of the form \\(\\eta_{i}=\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta}\\):\n\\[\\begin{align}\nl_{\\text{PO}}(\\boldsymbol{\\beta};\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{n};y_{1},\\ldots,y_{n}) &=\\sum_{i=1}^{n}\\left(y_{i}\\eta_{i}-e^{\\eta_{i}}-\\log_{e}\\left(y_{i}!\\right)\\right)\\\\\n&=\\sum_{i=1}^{n}\\left(y_{i}(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})-e^{\\log_{e}w_{i}}e^{\\mathbf{x}_{i}\\boldsymbol{\\theta}}-\\log_{e}\\left(y_{i}!\\right)\\right)\\\\\n&=\\sum_{i=1}^{n}\\left(y_{i}(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})-w_{i}e^{\\mathbf{x}_{i}\\boldsymbol{\\theta}}\\right)\n\\label{eq:poisson-reg}\n\\end{align}\\]\nNote that since \\(y_{i}\\in\\{0,1\\}\\), \\(y_{i}!=1\\Rightarrow\\log_{e}\\left(y_{i}!\\right)=0\\) hence we lose the last term in the second line. So letting \\(\\lambda(\\mathbf{x}_{i},\\boldsymbol{\\theta})=\\exp(\\log_{e}w_{i}+\\mathbf{x}_{i}\\boldsymbol{\\theta})\\) we have that (\\(\\ref{eq:poisson-reg}\\)) is equivalent to (\\(\\ref{eq:ppm-approx}\\)). Note that sometimes this derivation is via using weights rather than using the offset. This approach is generally referred to as the ‚ÄúBerman-Turner device‚Äù.\nSo to fit the inhomogeneous Poisson process model in (\\(\\ref{eq:pp-lik}\\)), we can fit a Poisson GLM with the following components:\n\nresponse vector \\(\\mathbf{y}=(\\overbrace{1,\\ldots,1}^{m\\text{ times}},\\overbrace{0,\\ldots,0}^{J\\text{ times}})\\),\ndesign matrix \\(\\mathbf{X}\\), where the first \\(m\\) rows are the data locations (and potentially associated covariates), then the following \\(J\\) rows are the locations of the quadrature points,\noffset vector of \\(1\\)s and quadrature weights, \\(\\mathbf{w}=(\\overbrace{1,\\ldots,1}^{m\\text{ times}},w_{1},\\ldots,w_{J})\\).\n\nGenerating an efficient and accurate quadrature scheme for the given problem can potentially be tricky. In one dimension generating a mesh for the quadrature is pretty simple, we can just make an evenly-spaced grid over space (or the range of the covariate). In higher dimensions with uneven data this can be more complicated. Warton and Shepherd (2010) suggest increasing grid complexity until convergence of the maximum likelihood estimate to ensure that the integral is approximated correctly. Simpson et al. (2016) suggest that regular grids are computationally wasteful and suggest the use of triangulation for efficiency."
  },
  {
    "objectID": "articles/poisson_processes.html#footnotes",
    "href": "articles/poisson_processes.html#footnotes",
    "title": "Poisson processes in mgcv",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGeneralized linear model, generalized linear mixed model and of course generalized additive model.‚Ü©Ô∏é\nThat is, where we describe the variation in space using structured random effects that are (multivariate) normal distributed.‚Ü©Ô∏é\nSee https://www.youtube.com/watch?v=4cia_v4vxfE.‚Ü©Ô∏é\nWritten by Finn Lindgren and available on CRAN.‚Ü©Ô∏é\nI‚Äôm using ‚Äúintegration grid‚Äù and ‚Äúmesh‚Äù interchangably here.‚Ü©Ô∏é\nAdapted from here‚Ü©Ô∏é\nA property of being something called a simple point process. Janine Illian once tried to explain this to me but I‚Äôve forgotten now. Sorry Janine!‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/gamvar.html",
    "href": "articles/gamvar.html",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "",
    "text": "This document aims to clear up which approaches are appropriate for estimation of variance of the parameters (and dervied quantities from those parameters) of generalized additive models. Three approaches are discussed: analytic, posterior simulation and bootstrap."
  },
  {
    "objectID": "articles/gamvar.html#making-predictions-with-a-gam",
    "href": "articles/gamvar.html#making-predictions-with-a-gam",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Making predictions with a GAM",
    "text": "Making predictions with a GAM\nWe are not usually interested in the variance of the coefficients (\\(\\boldsymbol{\\beta}\\)) themselves, but rather some derived quantity like predictions or summary statistics of predictions. We want to evaluate (\\(\\ref{eq:dsm}\\)) at (new) values of the covariates such that:\n\\[\\begin{equation}\nn_{m}^{*}=\\exp\\left(\\log_{e}A_{m}+\\beta_{0}+f_{\\text{space}}(\\texttt{lat}_{m},\\texttt{lon}_{m})+f_{\\text{depth}}(\\texttt{Depth}_{m})\\right),\\label{eq:dsm-1}\n\\end{equation}\\] In ‚ÄúPractical examples‚Äù below, I‚Äôll show some examples for count data, where the quantity of interest is abundance over some region: sums of predictions over some grid.\nBefore going into variance estimation, we first recap how predictions are formed in GAMs. We have estimated \\(\\hat{\\boldsymbol{\\beta}}\\) and want to make predictions at some new data locations. Prediction is usually explained as ‚Äúplugging-in‚Äù the new covariate values into equation (\\(\\ref{eq:gam}\\)) (or (\\(\\ref{eq:dsm}\\))). This is true, but doesn‚Äôt fully explain what‚Äôs going on. When we build our model, we form a design matrix (\\(\\mathbf{X}\\)), the rows of which correspond to observations and the columns correspond to the basis functions of the smooths (or just the covariates in the case of a GLM). To make a prediction we need to do the same thing again for these (\\(M\\)) new prediction locations, so we form \\(\\mathbf{X}_{p}\\), the (Wood, 2017), which will look something like: \\[\n\\mathbf{X}_{p}=\\left(\\begin{array}{ccccccc}\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},1}) & b_{\\texttt{a},2}(x_{\\texttt{a},1}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},1}) & b_{\\texttt{b},2}(x_{\\texttt{b},1}) & \\dotsc\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},2}) & b_{\\texttt{a},2}(x_{\\texttt{a},2}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},2}) & b_{\\texttt{b},2}(x_{\\texttt{b},2}) & \\dotsc\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & b_{\\texttt{a},1}(x_{\\texttt{a},M}) & \\dotsc & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & b_{\\texttt{b},1}(x_{\\texttt{b},M}) & \\dotsc\n\\end{array}\\right),\n\\] where the first column of 1s is for the intercept term and \\(b_{\\texttt{a},j}(x_{\\texttt{a},m})\\) is the evaluation of the \\(j\\)th basis function for covarite \\(\\texttt{a}\\), measured as \\(x_{\\texttt{a},i}\\) for prediction point \\(i\\). Mutliplying \\(\\mathbf{X}_{p}\\) by the estimated model coefficients (\\(\\hat{\\boldsymbol{\\beta}}\\)), we get the linear predictor \\(\\boldsymbol{\\eta}=\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\). We then need to apply the (inverse) link function (\\(g^{-1}\\)) to the linear predictor to get predictions on the correct scale (\\(\\mathbf{y}=g^{-1}\\left(\\boldsymbol{\\eta}\\right)=g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\)). We often want to summarize the resulting predictions, for example by summing them, so we calculate \\(y^{*}=\\sum_{m=1}^{M}y_{m}\\). We could also write this in matrix notation as \\(y^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) where \\(\\mathbf{1}\\) is a row-vector of 1s the same length as \\(\\textbf{y}\\). \\(\\mathbf{X}_{p}\\) is sometimes known as the ‚Äúprojection matrix‚Äù or ‚Äú\\(\\mathbf{L}_{p}\\) matrix‚Äù. It maps the estimated coefficients to the predicted values (on the link scale). Thinking of \\(\\mathbf{X}_{p}\\) as a way to move between the coefficients and derived model quantities (like predictions) is key to understanding how variance can be estimated in the cases below.\nA simple example shows how this works in mgcv:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(2)\ndat &lt;- gamSim(1,n=400,dist=\"normal\",scale=2)\n\nGu & Wahba 4 term additive model\n\n# fit a model\nb &lt;- gam(y~s(x0),data=dat)\n\n# prediction matrix\npred &lt;- data.frame(x0 = seq(0, 1, length.out=100))\n\n# build the Xp matrix\nXp &lt;- predict(b, pred, type=\"lpmatrix\")\n\n# compare \"manual\" generation of predictions to those from predict()\nmanual &lt;- Xp %*% coef(b)\nauto &lt;- predict(b, pred)\n\n# need to convert auto to be a matrix\nidentical(manual, as.matrix(auto))\n\n[1] TRUE"
  },
  {
    "objectID": "articles/gamvar.html#analytic-estimation",
    "href": "articles/gamvar.html#analytic-estimation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic estimation",
    "text": "Analytic estimation\nOnce we have fitted our model, we have not only the coefficients \\(\\hat{\\boldsymbol{\\beta}}\\), but also a posterior covariance matrix of those coefficients, \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\). What we‚Äôd really like is the variance of our linear predictor (\\(\\boldsymbol{\\eta}=\\mathbf{X}\\boldsymbol{\\beta}\\)) or some function of the linear predictor (\\(h(\\boldsymbol{\\eta})=h(\\mathbf{X}\\boldsymbol{\\beta}\\))). By the delta method (e.g., (Wasserman, 2004) or (Seber, 1987)), we know that we can calculate the variance of some function of the linear predictor as: \\[\\begin{align}\n\\text{Var}[h(\\boldsymbol{\\eta})] & =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\text{Var}(\\boldsymbol{\\beta})\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)\\\\\n& =\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial h(\\boldsymbol{\\eta})}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right), \\label{eq:delta-general}\n\\end{align}\\] where the derivative terms are the derivatives of \\(h(\\boldsymbol{\\eta})\\) with respect to the model parameters (\\(\\boldsymbol{\\beta}\\)) evaluated at their estimates. Intuitively we can think of this expression as rotating and rescaling the variance-covariance matrix of the mode l parameters first to the scale of the linear predictor and then into the space of \\(h\\)().\nFollowing on with our density surface example, if we wanted the variance of each prediction, in that case we think of the function \\(h\\) as just the link function \\(h(\\boldsymbol{\\eta})=g(\\boldsymbol{\\eta})\\). If we use a Tweedie for the response and therefore a \\(\\log_{e}\\) link, we have that \\(h(\\hat{\\boldsymbol{\\eta}})=\\text{exp}(\\hat{\\boldsymbol{\\eta}})=\\hat{\\mathbf{n}}\\) (if we start thinking in terms of the estimated model and \\(\\hat{\\mathbf{n}}\\) is a vector of estimated abundances). So we can then calculate: \\[\\begin{align}\n\\text{Var}(\\hat{\\mathbf{n}}) & =\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\mathbf{n}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varn}\n\\end{align}\\] here we rescaling the variance-covariance of \\(\\hat{\\boldsymbol{\\beta}}\\) to be that of the estimated counts. In this case \\(\\text{Var}(\\hat{\\mathbf{n}})\\) would be a vector of variances about the predictions (when calling predict(model, se.fit=TRUE, type=''response'') in mgcv, the $se.fit terms are \\(\\sqrt{\\text{Var}(\\hat{\\mathbf{n}})}\\) (Wood, 2017)) .\nIn our density surface model example, we might really want to know about \\(\\hat{N}=\\sum\\hat{N}_{i}\\) where \\(\\hat{N}_{i}\\) are predicted abundances over some grid and we wish to know the total abundance \\(\\hat{N}\\) and its variance \\(\\text{Var}(\\hat{N})\\):\n\\[\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\right),\\label{eq:delta-varNhat}\n\\end{align}\\] where we expand out \\(\\hat{N}=\\sum\\hat{N}_{i}=\\mathbf{A}g^{-1}\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\) to take the derivative where \\(\\mathbf{A}\\) is a row vector of prediction grid cell areas. Since we usually use a \\(\\log\\)-link, we end up with \\(g^{-1}(x)=\\exp(x)\\), so: \\[\\begin{align*}\n\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}} & =\\frac{\\partial}{\\partial\\boldsymbol{\\beta}}\\left[\\mathbf{A}\\exp\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}\\right)\\right]\\\\\n& =\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right),\n\\end{align*}\\] where \\(\\odot\\) indicates element-wise multiplication (so the \\(i,j\\)th entry in \\(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\) is \\(\\exp\\left(\\left[\\mathbf{X}_{p}\\right]_{.,j}\\hat{\\boldsymbol{\\beta}}\\right)\\left[\\mathbf{X}_{p}\\right]_{ij}\\), where \\(\\left[\\right]_{ij}\\) denotes the \\(i,j\\)th entry of a matrix). So, our final (messy) expression is: \\[\n\\begin{align}\n\\text{Var}(\\hat{N}) & =\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right)^\\text{T}\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\left(\\mathbf{A}\\left(\\exp\\left(\\mathbf{X}_{p}\\hat{\\boldsymbol{\\beta}}\\right)\\odot\\mathbf{X}_{p}\\right)\\right),\\label{eq:delta-varNhat-1}\n\\end{align}\n\\] This is the method implemented in dsm::dsm.var.gam, dsm::dsm.var.prop and dsm::dsm_varprop. Note that we could instead calculate \\(\\frac{\\partial\\hat{N}}{\\partial\\boldsymbol{\\beta}}\\Bigg\\vert_{\\boldsymbol{\\beta}=\\boldsymbol{\\hat{\\beta}}}\\) numerically by finite-differencing."
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation",
    "href": "articles/gamvar.html#posterior-simulation",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can also use the fact that fitting a GAM in mgcv is an empirical Bayes procedure, so we have a posterior distribution for \\(\\hat{\\boldsymbol{\\beta}}\\) given the smoothing parameter estimates, \\(\\hat{\\boldsymbol{\\lambda}}\\), (as described above). As an example see Figure Figure¬†1 where posterior samples of \\(\\hat{\\boldsymbol{\\beta}}\\) are used to construct multiple smooths, these smooths then follow the properties of the analytical estimates (based on Bayesian arguments by Marra and Wood (2012)). As can be seen from the plot, the generated grey curves mostly lie within the \\(\\pm2\\) standard error dashed lines.\n\n\n\n\n\n\n\n\nFigure¬†1: Fitted model (black lines) with confidence band (limits in dashed lines) generated from predict(..., se.fit=TRUE) along with 200 samples from the posterior of the model (grey).\n\n\n\n\n\nWe generate the grey curves by simply generating \\(\\boldsymbol{\\beta}_{b}\\sim N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), then multiplying by the prediction matrix (\\(\\mathbf{X}_{p}\\)) to obtain predictions on the linear predictor scale (then applying the link function if necessary). This approach can be particularly handy in the case where we want to calculate the variance of some summary of the linear predictor values. This is particularly useful as the link function is often a non-linear function of the linear predictor, so the approximation in (\\(\\ref{eq:delta-general}\\)) might not be appropriate.\nAs an alternative to (\\(\\ref{eq:delta-varNhat}\\)), the following algorithm can be used:\n\nFor \\(b=1,\\ldots,B\\):\nSimulate from \\(N(\\hat{\\boldsymbol{\\beta}},\\mathbf{V}_{\\hat{\\boldsymbol{\\beta}}})\\), to obtain \\(\\boldsymbol{\\beta}_{b}\\).\nCalculate predicted abundance for this \\(\\boldsymbol{\\beta}_{b}\\), \\(\\hat{N}_{b}^{*}=\\mathbf{1}g^{-1}\\left(\\mathbf{X}_{p}\\boldsymbol{\\beta}_{b}\\right)\\)\nStore \\(\\hat{N}_{b}^{*}\\).\nCalculate the empirical variance or percentiles of the \\(\\hat{N}_{b}\\)s.\n\nIn practice \\(B\\) does not have to be particularly large. (Marra et al., 2012) achieve reasonable results with \\(B=100\\)."
  },
  {
    "objectID": "articles/gamvar.html#bootstrapping",
    "href": "articles/gamvar.html#bootstrapping",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nA popular reply to the question of how to calculate uncertainty for complex models is ‚Äúdo a bootstrap‚Äù. What is meant by this is usually that one should resample the data with replacement, refitting models each time and calculating some summary statistic. This seems appealing but has been shown (Carlin and Gelfand, 1991; Laird and Louis, 1987) that the use of so-called ‚ÄúnaÔøΩve‚Äù bootstraps leads to underestimation of uncertainty. The issue stems from the fact that GAM terms are essentially structured random effects, which have priors on them. When data is resampled, the prior structure is ignored so the prior uncertainty is collapsed leaving only the sampling variation in the bootstrap resamples. (Bravington et al., 2018) show a simple simulated example of this happening. Simply put, the bootstrap makes the assumption that all possible distinct values of the covariates have been observed (Rubin, 1981).\n\n\nRasmus BÔøΩÔøΩth gives a good explanation of a non-parametric bootstrap from a Bayesian point of view here: [http://www.sumsar.net/blog/2015/04/the-non-parametric-bootstrap-as-a-bayesian-model/].\nIt‚Äôs not impossible to concoct some kind of special bootstrap that could deal with this situation, what is sure is that this would require some careful thinking in each situation. The previous two methods just work."
  },
  {
    "objectID": "articles/gamvar.html#analytic-method",
    "href": "articles/gamvar.html#analytic-method",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Analytic method",
    "text": "Analytic method\nTo make the analytic variance calculation given in (\\(\\ref{eq:delta-varNhat}\\)), we need the ingredients for that: \\(\\hat{\\boldsymbol{\\beta}}\\) (the estimated coefficients, \\(\\mathbf{X}_{p}\\) (the prediction matrix) and \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) (posterior variance-covariance matrix for \\(\\hat{\\boldsymbol{\\beta}}\\)). These are obtained as follows:\n\n# the coefficients\nbetas &lt;- m$coef\n# prediction matrix\nXp &lt;- predict(m, predgrid, type=\"lpmatrix\")\n# posterior variance-covariance for the betas\nVbeta &lt;- vcov(m, unconditional=TRUE)\n\nWe can then simply write-out the mathematics:\n\n# row vector of areas\nareas &lt;- matrix(predgrid$off.set, nrow=1)\n\n# calculate the \"bread\" (either side of our matrix \"sandwich\")\n# calculate this in 2 bits...\nexpxb &lt;- diag(exp(Xp%*%betas)[,1], ncol=nrow(Xp), nrow=nrow(Xp))\nbread &lt;- expxb %*% Xp\n\n# get the variance without the offset\nV_analytic &lt;- bread %*% Vbeta %*% t(bread)\n\n# multiply up by the offset\nV_analytic &lt;- areas %*% V_analytic %*% t(areas)\n\n# standard error\nsqrt(V_analytic)\n\n         [,1]\n[1,] 344.6067\n\n# CV\nsqrt(V_analytic)/Nhat\n\n          [,1]\n[1,] 0.1316786"
  },
  {
    "objectID": "articles/gamvar.html#posterior-simulation-1",
    "href": "articles/gamvar.html#posterior-simulation-1",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Posterior simulation",
    "text": "Posterior simulation\nWe can get to the same result by doing posterior simulation, using the variables betas, Xp and Vbeta from the above:\n\nset.seed(211)\n\n# number of samples\nnsamp &lt;- 200\n\n# storage for samples\nres &lt;- rep(0, nsamp)\n\nfor (i in 1:nsamp){\n  # sample from the posterior of the model parameters\n  br &lt;- matrix(rmvn(1, betas, Vbeta))\n  # make a prediction on the link scale\n  pr &lt;- Xp %*% br\n  # offset and transform, storing in res\n  res[i] &lt;- sum(predgrid$off.set*exp(pr))\n}\n\n# can do this quickly in 2 lines as\n# br &lt;- rmvn(nsamp, betas, Vbeta)\n# res &lt;- colSums(predgrid$off.set * exp(Xp %*% t(br)))\n\n# calculate the standard error\nsqrt(var(res))\n\n[1] 392.636\n\n# CV\nsqrt(var(res))/Nhat\n\n[1] 0.1500312\n\n\nNote that here we see that the CV (and standard error) is a little larger than for the delta method. In part because of the link function making extreme values more extreme (but only in one direction). We could get around this by implementing some kind of importance (or Metropolis-Hastings) sampler, ensuring that more values are sampled near ‚Äúrealistic‚Äù values or simply by increasing nsamp. Since we are sampling from a multivariate normal, the procedure is relatively fast and we can afford to make nsamp relatively large. The likely bottleneck in the code is the matrix multiplication when the two line method is used.\n\n\n\n\n\n\n\n\nFigure¬†3: Left: histogram of posterior samples of abundance for the sperm whale data. Right: posterior samples from the depth smooth."
  },
  {
    "objectID": "articles/gamvar.html#discussion",
    "href": "articles/gamvar.html#discussion",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Discussion",
    "text": "Discussion\nAbove I‚Äôve described three methods for estimating variance from GAMs, specifically with examples from spatial modelling of distance sampling data. It has been clear for some time that nonparametric bootstrap-based methods are not appropriate for models with random effects, but the alternatives have not been laid-out, especially for spatial data. Hopefully the examples here show how internal functions in dsm work, and how they can be implemented for those who don‚Äôt use that package.\nFor those dealing with DSMs, the above doesn‚Äôt directly address the issue of variance that comes from the detection function. Fortunately, including this source of uncertainty comes without too much additional effort. If we use the methods described in (Bravington et al., 2018), we can obtain a \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) that includes detection function uncertainty. We can then use that \\(\\boldsymbol{V}_{\\hat{\\boldsymbol{\\beta}}}\\) in the procedures outlined above."
  },
  {
    "objectID": "articles/gamvar.html#acknowledgements",
    "href": "articles/gamvar.html#acknowledgements",
    "title": "Model parameter uncertainty estimation approaches for generalized additive models",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work was originally funded by OPNAV N45 and the SURTASS LFA Settlement Agreement, and being managed by the U.S. Navy‚Äôs Living Marine Resources program under Contract No.¬†N39430-17-C-1982."
  },
  {
    "objectID": "articles/ncv_timeseries.html",
    "href": "articles/ncv_timeseries.html",
    "title": "Neighbourhood cross-validation for time series",
    "section": "",
    "text": "This is the second in a series on neighbourhood cross-validation, following from the introductory article here. The neighbourhood cross-validation fitting method is available in mgcv version 1.9-0 onwards. Here I‚Äôll focus on using cross-validation as a replacement for using autoregressive modelling structures when dealing with temporally correlated data.\nThe primary reference for the NCV method is Simon Wood‚Äôs arXiv preprint.\nIt‚Äôs important to note that I know very little about time series analysis, the focus here is on the methods we can use to fit things with NCV. There may well be much better ways to analyse these time series! Don‚Äôt yell at me.\nThis is based on some work with BioSS colleagues: Katherine Whyte, Ana Couto and Thomas Cornulier. Thanks to them for their input at various stages of this."
  },
  {
    "objectID": "articles/ncv_timeseries.html#i-have-a-time-series-now-im-sad",
    "href": "articles/ncv_timeseries.html#i-have-a-time-series-now-im-sad",
    "title": "Neighbourhood cross-validation for time series",
    "section": "I have a time series, now I‚Äôm sad",
    "text": "I have a time series, now I‚Äôm sad\nWe‚Äôll start by looking at a high-resolution time series collected by the fine folks at the UK Centre for Ecology and Hydrology1. Data are from the COSMOS experiment, which collects soil moisture data around the UK, using literally cosmic rays (fast neutrons, see here for more information). These data are collected at about 50 sites across the UK at a temporal resolution of every 15 minutes.\nWe will use the COSMOS-UK soil moisture data that has been aggregated at a daily resolution (mainly for computational time reasons). Note also that this is not the up-to-date version of the data, it‚Äôs based on when I downloaded it so the data ranges from mid-2014 through to the end of 2022.\nThe measurement we‚Äôre interested in here is the ‚Äúvolumetric water content‚Äù (VWC) as a function of other covariates such as space and time. VWC is derived from counts of fast neutrons in the surrounding area by a cosmic-ray sensing probe.\nI‚Äôve previously processed the data, the script for which you can find here. You can download the full, processed dataset here.\nLoading the data, we have two data.frames: all_sites which has the full data set and uk_grid which is a prediction grid (which we‚Äôll ignore here). Taking a peak at the all_sites data:\n\nload(\"cosmos-processed.RData\")\nhead(all_sites)\n\n  SITE_ID  SITE_NAME LONGITUDE LATITUDE NORTHING EASTING ALTITUDE\n1   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n2   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n3   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n4   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n5   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n6   ALIC1 Alice Holt -0.858232 51.15355   139985  479950       80\n          LAND_COVER    SOIL_TYPE  DATE_TIME PRECIP SNOW_DEPTH COSMOS_VWC ndate\n1 Broadleaf woodland Mineral soil 2015-03-07    0.0         NA       40.8 16501\n2 Broadleaf woodland Mineral soil 2015-03-08    0.0         NA       42.9 16502\n3 Broadleaf woodland Mineral soil 2015-03-09    0.1         NA       38.8 16503\n4 Broadleaf woodland Mineral soil 2015-03-10    0.0         NA       45.9 16504\n5 Broadleaf woodland Mineral soil 2015-03-11    0.0         NA       39.7 16505\n6 Broadleaf woodland Mineral soil 2015-03-12    0.0         NA       42.1 16506\n  month year\n1     3 2015\n2     3 2015\n3     3 2015\n4     3 2015\n5     3 2015\n6     3 2015\n\n\nWe won‚Äôt confuse things by trying to deal with all the sites here, so let‚Äôs just select one: Balruddery (which is over the road from my office) near Dundee. Let‚Äôs take a look at that time series:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nbal &lt;- subset(all_sites, SITE_NAME==\"Balruddery\")\n\nggplot(bal) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also plot this as years on top of each other, to get a feeling of how similar the series is between years.\n\nlibrary(lubridate)\nggplot(bal) +\n  geom_line(aes(x=DATE_TIME-ymd(paste0(year, \"-01-01\")),\n                y=COSMOS_VWC,\n                group=as.factor(year),\n                colour=as.factor(year))) +\n  labs(x=\"Date\", y=\"Volumetric water content\", colour=\"Year\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also look at partial autocorrelation plots using the built-in pacf2 function in R. Plotting this for the VWC shows the autocorrelation in the data.\n\npacf(bal$COSMOS_VWC, lag.max=30, main=\"Partial autocorrelogram for Balruddery\")\n\n\n\n\n\n\n\n\nTwo things to think about here: - I‚Äôve increased lag.max from its default, so we can see 30 lags. - The partial autocorrelation function is the right thing to look at here (I think) because we don‚Äôt want to look at every lag between each time point: we really want to see the autocorrelation between a given point and future data."
  },
  {
    "objectID": "articles/ncv_timeseries.html#lets-fit-a-dumb-model-to-one-year-and-see-whats-happening",
    "href": "articles/ncv_timeseries.html#lets-fit-a-dumb-model-to-one-year-and-see-whats-happening",
    "title": "Neighbourhood cross-validation for time series",
    "section": "Let‚Äôs fit a dumb model to one year and see what‚Äôs happening",
    "text": "Let‚Äôs fit a dumb model to one year and see what‚Äôs happening\nStarting by looking at Balruddery in 2019, we can try to fit a ‚Äústandard‚Äù GAM to the data and see what happens.\nBefore we jump into modelling, let‚Äôs take a look at the autocorrelation in 2019 only:\n\n# just get the 2019 data\nbal_2019 &lt;- subset(bal, year(DATE_TIME) == 2019)\n\npacf(bal_2019$COSMOS_VWC, lag.max=30, main=\"Partial autocorrelogram for Balruddery 2019\")\n\n\n\n\n\n\n\n\nWe‚Äôll only use the date to explain the model. The ndate variable is a numeric version of the date information.\n\nlibrary(mgcv)\nb0 &lt;- gam(COSMOS_VWC ~ s(ndate, k=20), data=bal_2019, method=\"REML\")\nsummary(b0)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nCOSMOS_VWC ~ s(ndate, k = 20)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.2112     0.1259   255.9   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(ndate) 14.87  17.11 22.55  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.517   Deviance explained = 53.7%\n-REML = 865.23  Scale est. = 5.7842    n = 365\n\n\nNote that I‚Äôve bumped-up the maximum basis size. I could have increased it a lot more for this basic GAM, since it‚Äôs trying to fit to all that variation in the data. This is one of the many issues with fitting a ‚Äúsimple‚Äù model to this kind of high-resolution time series data ‚Äì our usual ideas of how to increase complexity don‚Äôt make sense. We want to get to the underlying pattern for the year, rather than getting hung-up on short-scale phenomena.\nCheck plots show pretty good behaviour:\n\ngam.check(b0)\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 6 iterations.\nGradient range [-2.903424e-06,1.933717e-07]\n(score 865.2256 & scale 5.784242).\nHessian positive definite, eigenvalue range [3.383494,181.77].\nModel rank =  20 / 20 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n           k'  edf k-index p-value    \ns(ndate) 19.0 14.9    0.32  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs mentioned we could be increasing k but we won‚Äôt for now. Perhaps the tails of the residual distribution seem a little fat, but we won‚Äôt worry too much about this for our straw man model.\nLooking at the fit:\n\n# normally we'd need to construct a prediction data set here and use\n# the predict() function's newdata= argument, but since we have data for\n# each day, we don't need to supply that value\ntemp &lt;- predict(b0, se=TRUE)\nbal_2019$pred_b0 &lt;- temp$fit\nbal_2019$pred_b0_u &lt;- temp$fit + 1.96*temp$se.fit\nbal_2019$pred_b0_l &lt;- temp$fit - 1.96*temp$se.fit\n\n# note that we can automate this using the pred_with_ci function\n# in the mgcvUtils package. We'll use that next time...\n\nggplot(bal_2019) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b0), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b0_u, ymin=pred_b0_l),\n              fill=\"red\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis still looks very wiggly and I don‚Äôt think this really explains the overall yearly pattern in the data very well ‚Äì we‚Äôre overfitting."
  },
  {
    "objectID": "articles/ncv_timeseries.html#autoregression-via-a-gamm",
    "href": "articles/ncv_timeseries.html#autoregression-via-a-gamm",
    "title": "Neighbourhood cross-validation for time series",
    "section": "Autoregression via a GAMM",
    "text": "Autoregression via a GAMM\nThe gamm function in mgcv will allow us to fit an autoregressive process in addition to the smoother. This should handle some of the short-scale autocorrelation in the data. Here we‚Äôll use an AR(\\(p\\)) process ‚Äì allowing the previous \\(p\\) time points to influence the following point. This is a ‚Äúusual‚Äù way of dealing with data that has autocorrelation.\nBased on the pacf plot above, let‚Äôs try \\(p=10\\):\n\nb1 &lt;- gamm(COSMOS_VWC ~ s(ndate, k=20), data=bal_2019,\n           correlation=corARMA(p=10))\nsummary(b1$gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nCOSMOS_VWC ~ s(ndate, k = 20)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   32.217      0.433   74.41   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df    F p-value    \ns(ndate) 2.694  2.694 8.06 0.00053 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.338   \n  Scale est. = 8.2245    n = 365\n\n\nNote that the effective degrees of freedom have decreased from 15.87 to 3.69, so we expect that the resulting smooth will be much‚Ä¶ smoother :)\nWe‚Äôll use the pred_with_ci function for speed in generating confidence intervals, it‚Äôs from the mgcvUtils package, available on github here.\n\n# install mgcvUtils\ndevtools::install_github(\"dill/mgcvUtils\")\n\nUsing github PAT from envvar GITHUB_TOKEN. Use `gitcreds::gitcreds_set()` and unset GITHUB_TOKEN in .Renviron (or elsewhere) if you want to use the more secure git credential store instead.\n\n\nSkipping install of 'mgcvUtils' from a github remote, the SHA1 (c36c58ac) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nlibrary(mgcvUtils)\n\nbal_2019 &lt;- pred_with_ci(b1$gam, bal_2019,\n                         pred_label=\"pred_b1\", ci_label=\"pred_b1\")\n\nggplot(bal_2019) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b1), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b1_u, ymin=pred_b1_l),\n              fill=\"red\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThat‚Äôs a lot smoother. We have an AR parameters of:\n\nb1$lme$modelStruct$corStruct\n\nCorrelation structure of class corARMA representing\n        Phi1         Phi2         Phi3         Phi4         Phi5         Phi6 \n 0.799478706 -0.020660275 -0.008209886  0.041636043  0.005374399 -0.066563555 \n        Phi7         Phi8         Phi9        Phi10 \n 0.029103710 -0.020105682  0.031664372 -0.003268618 \n\n\nindicating strong positive correlation between the data at a lag of 1, then smaller correlations at further lags."
  },
  {
    "objectID": "articles/ncv_timeseries.html#using-neighbourhood-cross-validation",
    "href": "articles/ncv_timeseries.html#using-neighbourhood-cross-validation",
    "title": "Neighbourhood cross-validation for time series",
    "section": "Using neighbourhood cross-validation",
    "text": "Using neighbourhood cross-validation\nFrom the paper:\nIf it is reasonable to assume that there is short range residual correlation between point \\(k\\) and points in \\(\\alpha(k)\\), but not between \\(k\\) and \\(j \\in / \\alpha(k)\\), then NCV provides a means to choose hyper-parameters without the danger of overfit that such short range (positive) autocorrelation otherwise causes\nThe big question, then, is ‚Äúwhat does \\(\\alpha(k)\\) look like for my data‚Äù?\nLet‚Äôs start by using the autocorrelogram as a guide, and let‚Äôs also be a bit pessimistic and say that the neighbourhood goes out to lag 40 around each point (20 in each direction).\nA reminder from the documentation:\n`k` is the vector of indices to be dropped for each neighbourhood and `m`\ngives the end of each neighbourhood. So `nei$k[(nei$m[j-1]+1):nei$m[j]]`\ngives the points dropped for the neighbourhood `j`. `i` is the vector of\nindices of points to predict, with corresponding endpoints `mi`. So\n`nei$i[(nei$mi[j-1]+1):nei$mi[j]]` indexes the points to predict for\nneighbourhood j.\nWe‚Äôre going to create a moving-window-type approach, for the 20 observations either side of each datum. In the literature, this is often referred to as ‚Äúblocked CV‚Äù (Liu and Zhou, 2024; Snijders, 1988). We can do that relatively easily (though maybe somewhat cryptically) using the below code.\n\nnei &lt;- list()\nstart &lt;- pmax(1, (1:365)-20)\nend &lt;- pmin(365, (0:364)+20)\nnt &lt;- lapply(1:365, \\(x) start[x]:end[x])\nnei$k &lt;- unlist(nt)\nnei$m &lt;- cumsum(lapply(nt, length))\nnei$i &lt;- nei$k\nnei$mi &lt;- nei$m\n\nOne issue with setting these neighbourhoods up is that we aren‚Äôt 100% sure whether the computer is doing what we want. I made a wee tool to visualise this for time series (at least) to check what the cross-validation structure you created looks right. You can find that as part of the mgcvUtils package on github here. We can then pass the neighbourhood structure to it and see what it produces:\n\nvis_nei(nei)\n\n\n\n\n\n\n\n\nThe horizontal axis indexes the data, the vertical gives the fold. So for the first fold (bottom row) we see the 20 observations after the first, with the window increasing its width until we get a full 40 data included in the block. We can see here that the elements to drop and predict (‚ÄúBoth‚Äù) form a band over the data3. The uncoloured part of the data is used to fit the model.\nNow we‚Äôre happy with the neighbourhood structure, we can proceed to model fitting.\n\nb2 &lt;- gam(COSMOS_VWC ~ s(ndate, k=20), data=bal_2019, method=\"NCV\", nei=nei)\nsummary(b2)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nCOSMOS_VWC ~ s(ndate, k = 20)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.2112     0.1278     252   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(ndate) 17.47  18.69 20.95  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.525   Deviance explained = 54.8%\nNCV = 2181.3  Scale est. = 5.6974    n = 365\n\nbal_2019 &lt;- pred_with_ci(b2, bal_2019,\n                         pred_label=\"pred_b2\", ci_label=\"pred_b2\")\n\nggplot(bal_2019) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b2), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b2_u, ymin=pred_b2_l),\n              fill=\"red\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWell that looks quite nice! Note the reduced confidence band width!\nLet‚Äôs plot all three models together (with a bit of data fiddling to get that right):\n\nbal_2019_base &lt;- data.frame(DATE_TIME=bal_2019$DATE_TIME,\n                            pred = bal_2019$COSMOS_VWC,\n                            model = \"Observed VWC\",\n                            ci_u = NA, ci_l=NA)\n\nbal_2019_plot &lt;- rbind(bal_2019_base,\n                       pred_with_ci(b0, bal_2019_base, label=\"Simple GAM\",\n                                    label_label=\"model\"),\n                       pred_with_ci(b1$gam, bal_2019_base, label=\"GAMM\",\n                                    label_label=\"model\"),\n                       pred_with_ci(b2, bal_2019_base, label=\"NCV GAM\",\n                                    label_label=\"model\"))\n\nggplot(bal_2019_plot) +\n  geom_line(aes(x=DATE_TIME, y=pred, group=model, colour=model),\n            lwd=0.7) +\n  geom_ribbon(aes(x=DATE_TIME, ymax=ci_u, ymin=ci_l,\n                  group=model, fill=model),\n              alpha=0.5) +\n  labs(colour=\"Model\", fill=\"Model\", x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal() +\n  theme(legend.position=\"bottom\")\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf"
  },
  {
    "objectID": "articles/ncv_timeseries.html#what-about-other-cross-validation-schemes",
    "href": "articles/ncv_timeseries.html#what-about-other-cross-validation-schemes",
    "title": "Neighbourhood cross-validation for time series",
    "section": "What about other cross-validation schemes?",
    "text": "What about other cross-validation schemes?"
  },
  {
    "objectID": "articles/ncv_timeseries.html#short-term-forward-prediction",
    "href": "articles/ncv_timeseries.html#short-term-forward-prediction",
    "title": "Neighbourhood cross-validation for time series",
    "section": "Short-term, forward prediction",
    "text": "Short-term, forward prediction\nInstead of having a moving window around each datum, we could instead try a predict-forward scheme where we take data up to a given month and predict one month ahead (ignoring any future data). This means that as we work through the timeseries, we have more information (for the last set we have January-November data to work with).\n\nmdays &lt;- seq.Date(ymd(\"2019-01-01\"), ymd(\"2019-12-31\"), by=day(1))\n\nWarning: tz(): Don't know how to compute timezone for object of class numeric;\nreturning \"UTC\".\n\nmdays &lt;- month(ymd(mdays))\n\nnei &lt;- list()\nnei$k &lt;- c()\nnei$m &lt;- c()\n\n# let's write this as a for loop because it's easy to think about\nfor(i in 1:11){\n  # drop days after in this month\n  iind &lt;- which(mdays &gt; i)\n  # drop\n  nei$k &lt;- c(nei$k, iind)\n  nei$m &lt;- c(nei$m, length(iind))\n\n  # want to predict forward 1 month\n  iind &lt;- which(mdays == (i+1))\n  nei$i &lt;- c(nei$i, iind)\n  nei$mi &lt;- c(nei$mi, length(iind))\n}\nnei$m &lt;- cumsum(nei$m)\nnei$mi &lt;- cumsum(nei$mi)\n\nWhat does that look like?\n\nvis_nei(nei)\n\n\n\n\n\n\n\n\n(Note the difference in the plot compared to the previous examples where we only had one colour, now we have differing leave-out and prediction sets.)\nNow fitting that model and making predictions as before:\n\nb3 &lt;- gam(COSMOS_VWC ~ s(ndate, k=20), data=bal_2019, method=\"NCV\", nei=nei)\nsummary(b3)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nCOSMOS_VWC ~ s(ndate, k = 20)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.2112     0.1278     252   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(ndate) 17.47  18.69 20.95  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.525   Deviance explained = 54.8%\nNCV = 2181.3  Scale est. = 5.6974    n = 365\n\nbal_2019 &lt;- pred_with_ci(b3, bal_2019,\n                         pred_label=\"pred_b3\", ci_label=\"pred_b3\")\n\nggplot(bal_2019) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b3), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b3_u, ymin=pred_b3_l),\n              fill=\"red\", alpha=0.5) +\n  geom_line(aes(x=DATE_TIME, y=pred_b2), colour=\"#7cae00\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b2_u, ymin=pred_b2_l),\n              fill=\"#7cae00\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI‚Äôve plotted the b2 fit here in green for comparison. They look quite similar, but there is a difference in how big the May-ish dip is."
  },
  {
    "objectID": "articles/ncv_timeseries.html#what-about-restricting-the-prediction-neighbourhood",
    "href": "articles/ncv_timeseries.html#what-about-restricting-the-prediction-neighbourhood",
    "title": "Neighbourhood cross-validation for time series",
    "section": "What about restricting the prediction neighbourhood?",
    "text": "What about restricting the prediction neighbourhood?\nTaking the scheme that we saw in b2 again, but this time just predicting back for the middle datum in the dropped set, we might expect to see some difference in the results.\n\n# construct this the other way around, we want to predict each point:\nnei &lt;- list()\nnei$i &lt;- 1:365 #partygirl\nnei$mi &lt;- 1:length(start)\n\n# now take the 20 either side\nnt &lt;- lapply(nei$i, \\(x){\n  max(c(1, x-20)):min(c(365, x+20))\n})\n\nnei$k &lt;- unlist(nt)\nnei$m &lt;- cumsum(lapply(nt, length))\n# let the set we're predicting to be just the datum in the middle of the\n# training set\n\n# this is a bit wonky at the edges\nvis_nei(nei)\n\n\n\n\n\n\n\n\nNow looking at the results:\n\nb4 &lt;- gam(COSMOS_VWC ~ s(ndate, k=20), data=bal_2019, method=\"NCV\", nei=nei)\nsummary(b4)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nCOSMOS_VWC ~ s(ndate, k = 20)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  32.2112     0.1278     252   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F p-value    \ns(ndate) 17.47  18.69 20.95  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.525   Deviance explained = 54.8%\nNCV = 2181.3  Scale est. = 5.6974    n = 365\n\nbal_2019 &lt;- pred_with_ci(b4, bal_2019,\n                         pred_label=\"pred_b4\", ci_label=\"pred_b4\")\n\nggplot(bal_2019) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b4), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b4_u, ymin=pred_b4_l),\n              fill=\"red\", alpha=0.5) +\n  geom_line(aes(x=DATE_TIME, y=pred_b2), colour=\"#7cae00\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b2_u, ymin=pred_b2_l),\n              fill=\"#7cae00\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAgain, I‚Äôve plotted b2 in green for comparison. We again see a very similar smooth but the uncertainty band is much wider until later in the dataset. We might think this is more realistic (the earlier months are much more variable than the latter ones), but we need a bit more thought to make sense of this‚Ä¶\nWhat does this mean? Why are these results different? Can we gain some insights and use this to help construct cross-validation schemes in future?"
  },
  {
    "objectID": "articles/ncv_timeseries.html#what-does-cross-validation-do",
    "href": "articles/ncv_timeseries.html#what-does-cross-validation-do",
    "title": "Neighbourhood cross-validation for time series",
    "section": "What does cross-validation do?",
    "text": "What does cross-validation do?\nGiven cross-validation is a relatively ‚Äúsimple‚Äù idea: leave data out, fit a model, and predict, it‚Äôs actually a bit troubling that its not 100% obvious what it‚Äôs doing in terms of fitting in a completely statistical sense ‚Äì what are the predictions representative of? Fortunately, there‚Äôs a nice paper for that.\nBates et al. (2024) explain what cross-validation is doing internally and what it is that the CV score is estimating4. They find that it estimates ‚Äúthe average prediction error of models fit on other unseen training sets drawn from the same population.‚Äù That‚Äôs contrary to what we might think, that the estimate if of ‚Äúthe prediction error for the model at hand, fit to the training data.‚Äù\nLet‚Äôs unpack that a little and then apply it to the results we have above. If we‚Äôre taking subsets of the data fitting to them and then predicting back on those same sets, then the cross-validation score is estimating the average prediction error on the training sets (which weren‚Äôt included in the testing set used to fit the data). For our first example with NCV (b2), where we had a blocked, moving window design to our cross-validation scheme, we can see that we have a model that generally does a reasonable ‚Äúaverage‚Äù over the time series.\nOur ‚Äúshort-term, forward prediction‚Äù model (b3) looks like it‚Äôs overfitting a little to the late April dip in water content. Since the folds are by month, it seems that the model overfits to that dip:\n\nggplot(subset(bal_2019, month(DATE_TIME) %in% 4:5)) +\n  geom_line(aes(x=DATE_TIME, y=COSMOS_VWC)) +\n  geom_line(aes(x=DATE_TIME, y=pred_b3), colour=\"red\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b3_u, ymin=pred_b3_l),\n              fill=\"red\", alpha=0.5) +\n  geom_line(aes(x=DATE_TIME, y=pred_b2), colour=\"#7cae00\") +\n  geom_ribbon(aes(x=DATE_TIME, ymax=pred_b2_u, ymin=pred_b2_l),\n              fill=\"#7cae00\", alpha=0.5) +\n  labs(x=\"Date\", y=\"Volumetric water content\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe make our predictions for April and can‚Äôt help but fit as well as possible within each month (conditional on the overall smoothing parameter value, we still have to be smooth, but we can get a bit closer to the data).\nFor the b4, the modification of b2 where we only predict back to the middle value, we see a much wider confidence/credible band around the mean prediction. Again, thinking about this in terms of minimizing average prediction error helps us make sense of what‚Äôs happening here. For a larger prediction neighbourhood, some things come out in the wash better: we overfit somewhere but that underfits somewhere else, so the result for that fold isn‚Äôt as bad. When we predict only 1 point (and don‚Äôt include the autocorrelated data around it) we become more uncertain, since each of those differences between the fitted and observed values5 are then considered alone, without the neighbours ‚Äúhelping‚Äù to average things out.\nThinking about the ‚Äúreal‚Äù objective of NCV (average prediction error on unseen data) helps us understand what‚Äôs going on here.\nHopefully this is a helpful lesson in thinking about how NCV works and how the schemes used to fit the model can give rather different answers."
  },
  {
    "objectID": "articles/ncv_timeseries.html#footnotes",
    "href": "articles/ncv_timeseries.html#footnotes",
    "title": "Neighbourhood cross-validation for time series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have a half appointment at UKCEH, they really are nice folks‚Ü©Ô∏é\nNote that the pacf function assumes that your data are in order.‚Ü©Ô∏é\nIf we had disjoint sets for the leave out and predict sets, we‚Äôd see additional colours and pattern here. See later on for an example.‚Ü©Ô∏é\nThey also explain a lot of other stuff, it‚Äôs a nice paper!‚Ü©Ô∏é\nWell, actually we should think in terms of the model deviance if we‚Äôre really trying to think like the model.‚Ü©Ô∏é"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing material",
    "section": "",
    "text": "Yaaaay!\nYou have an example of a thing you can do in mgcv? You‚Äôd like to share that? That‚Äôs awesome.\nThe easiest way to do that is to submit a pull request to the site github or suggest a topic via the issue tracker."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yes! You can do that in mgcv",
    "section": "",
    "text": "This is a website about the kinds of things you can do in mgcv that you might not have realised that you can do in mgcv.\nHere are some possible topics:\n\nPoisson process fitting\ncross-validation\nstochastic partial differential equation smoothing\ncapture-recapture (I think?)\nbeing a secret Bayesian and being a more obvious Bayesian\n\nwith bonus material on:\n\nhow to do things not in mgcv (how to import mgcv smoothers into other models/frameworks/software)"
  },
  {
    "objectID": "articles/ncv.html",
    "href": "articles/ncv.html",
    "title": "Neighbourhood cross-validation",
    "section": "",
    "text": "This article is designed as a gentle introduction to the neighbourhood cross-validation fitting method available in mgcv version 1.9-0 onwards. Here I‚Äôll focus on the theory of how this stuff works and I‚Äôll try to write some further articles on the practical side of implementing this in particular situations for those interested in that kind of thing.\nThe primary reference for this is Simon Wood‚Äôs arXiv preprint, along with some discussions with Simon and others about various stuff. Thanks to those who listened to me yakk-on about this."
  },
  {
    "objectID": "articles/ncv.html#setting-up-a-neighbourhood-structure",
    "href": "articles/ncv.html#setting-up-a-neighbourhood-structure",
    "title": "Neighbourhood cross-validation",
    "section": "Setting-up a neighbourhood structure",
    "text": "Setting-up a neighbourhood structure\nLet‚Äôs define a goofy neighbourhood structure, where we just have four bits, in the order of one of the covariates.\nBegin with the list we need to store everything:\n\nnb &lt;- list()\n\nThis object will contain our \\(\\alpha(k)\\) and \\(\\delta(k)\\) information (albeit in a slightly convoluted way).\nThe leave-outs (\\(\\alpha(k)\\)‚Äôs) are in the k element of the list. This is a vector of data indices in such an order that each of our cross-validation blocks is contiguous. If our first block is size b then we can get the block indices via k[1]:k[b].\nIn our case we want the variable x0 chopped into 4 equal-sized, contiguous blocks of length 100. We can do this by getting the order of x0:\n\nnb$k &lt;- order(dat$x0)\n\nNext we need to set the end points of the blocks in the element m, which in our case will be 100, 200, 300 and 400:\n\nnb$m &lt;- (1:4)*100\n\nFor the predictions (\\(\\delta(k)\\)‚Äôs) we have a similar structure, k is now i and m is now mi (these should be the same length).\nHere let‚Äôs try to predict only at the points we dropped each time. In that case i == k:\n\nnb$i &lt;- nb$k\n\nand mi == m\n\nnb$mi &lt;- nb$m\n\nWe can now try to fit that by supplying nb as the nei= argument:\n\nb2 &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3),data=dat, method=\"NCV\", nei=nb)\nsummary(b2)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x0) + s(x1) + s(x2) + s(x3)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.8333     0.1005   77.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n        edf Ref.df      F  p-value    \ns(x0) 2.484  3.096  6.533 0.000229 ***\ns(x1) 2.438  3.029 83.167  &lt; 2e-16 ***\ns(x2) 7.701  8.567 94.181  &lt; 2e-16 ***\ns(x3) 1.000  1.000  3.588 0.058963 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.715   Deviance explained = 72.5%\nNCV = 1618.9  Scale est. = 3.9025    n = 400\n\n\nNote that the NCV score has changed (of course).\nLet‚Äôs plot that but showing the groupings in the rug plot, colour-coded:\n\npar(mfrow=c(2,2))\n\nblerg &lt;- lapply(1:4, function(x){\n  plot(b2, select=x, rug=FALSE)\n  rug(dat[[paste0(\"x\", x-1)]][nb$k[1:nb$m[1]]], col=\"red\")\n  rug(dat[[paste0(\"x\", x-1)]][nb$k[(nb$m[1]+1):nb$m[2]]], col=\"green\")\n  rug(dat[[paste0(\"x\", x-1)]][nb$k[(nb$m[2]+1):nb$m[3]]], col=\"blue\")\n  rug(dat[[paste0(\"x\", x-1)]][nb$k[(nb$m[3]+1):nb$m[4]]], col=\"pink\")\n})\n\n\n\n\n\n\n\n\nPerformance here is probably good because of the jumbled nature of the other covariates in the model ensuring good fit."
  },
  {
    "objectID": "articles/ncv.html#footnotes",
    "href": "articles/ncv.html#footnotes",
    "title": "Neighbourhood cross-validation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is equivalent to AIC and Mallow‚Äôs \\(C_p\\), see Stone (1977).‚Ü©Ô∏é\nAlso called the projection matrix or influence matrix. I like hat matrix, because it‚Äôs what puts the hats on the predictions ‚Äì it‚Äôs the matrix that maps the data \\(\\mathbf{y}\\), say, to it‚Äôs predictions \\(\\hat{\\mathbf{y}}\\), so \\(\\hat{\\mathbf{y}} = A\\mathbf{y}\\).‚Ü©Ô∏é\nSum of diagonal elements: \\(\\sum_j A_{jj}\\) or in R sum(diag(A)).‚Ü©Ô∏é\nI‚Äôll write something about P-IRLS and all this stuff at some point.‚Ü©Ô∏é\nNote that there‚Äôs still a major issue with methods like GCV (‚Äúprediction error based methods‚Äù, as opposed to likelihood-based methods like REML): they tend to be more sensitive to underfitting, so models end up more smooth than they should. See, e.g., Reiss and Ogden (2009).‚Ü©Ô∏é\nThings are a bit more complicated than this, but this is the basic idea. I will hopefully at some point write something about that.‚Ü©Ô∏é\nMore generally, this can be the loss function for the problem, so we can address a broader set of models, including e.g., quantile regression.‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/adaptive_smoothing.html",
    "href": "articles/adaptive_smoothing.html",
    "title": "Adaptive smoothing in mgcv",
    "section": "",
    "text": "When we set-up a smoother in a GAM, we have a penalty that is of the following matrix form: \\[\\begin{equation}\n\\lambda \\boldsymbol{\\beta}^\\intercal \\mathbf{S} \\boldsymbol{\\beta}.\n\\label{pen}\n\\end{equation}\\] where we have the fixed elements (usually integrals of derivatives of the basis functions) in \\(\\mathbf{S}\\) and the smoothers‚Äô coefficients in \\(\\boldsymbol{\\beta}\\). is the smoothing parameter which we also estimate and controls the influence of the penalty on the fit (usually, we have the above expression substracted from the restricted or marginal likelihood for REML or ML).\nThe smoothing parameter, \\(\\lambda\\), tells us about how wiggly to make our functions. A \\(\\lambda\\) of 0 will mean that the whole penalty term is evaluated as zero, meaning the smother will overfit to the data. Letting \\(\\lambda\\) get larger and larger, we end up with the penalty dominating the fit. In that case we retrieve only the nullspace terms: those bits of the basis which don‚Äôt have derivatives (so aren‚Äôt affected by the penalty).\nThis all works well in general, up to a point. What happens when the amount of smoothing we need changes with the covariate? In other words: what if we need the amount of smoothing to be adaptive?\nMost of this is based off Sections 5.33 and 5.3.5 of SN Wood (2017) as well as the ?smooth.construct.ad.smooth.spec manual page, with a bit more explanation. See the last section for links to other approaches."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#multiple-penalties",
    "href": "articles/adaptive_smoothing.html#multiple-penalties",
    "title": "Adaptive smoothing in mgcv",
    "section": "Multiple penalties",
    "text": "Multiple penalties\nFirst we need to build a penalty for such a term. We can extend \\(\\eqref{pen}\\) to the case where we have multiple penalties for a single term. In which case we write \\[\\begin{equation}\n  \\sum_j \\lambda_j \\boldsymbol{\\beta}^\\intercal \\mathbf{S}_j \\boldsymbol{\\beta},\n\\label{multpen}\n\\end{equation}\\] where we have split the penalty matrices and smoothing parameters and indexed them by \\(j\\). This idea should be fairly familiar as it‚Äôs how we create the total penalty for models with multiple terms. Just in this case, we‚Äôre using it for a single term."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#p-splines",
    "href": "articles/adaptive_smoothing.html#p-splines",
    "title": "Adaptive smoothing in mgcv",
    "section": "P-splines",
    "text": "P-splines\nWe need a basis to build our terms. For efficiency and interpretability purposes, mgcv uses P-splines (Eilers and Marx, 1996) to construct adaptive smoothers. Since they‚Äôre a bit different from other splines, let‚Äôs just review them for a moment.\n‚ÄúP-splines‚Äù in the sense used in mgcv are Marx and Eiler‚Äôs ‚ÄúB-splines‚Äù: they are a B-spline basis with a ‚Äúdifference‚Äù penalty (rather than an integral penalty). This makes things much faster for computation."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#b-spline-basis",
    "href": "articles/adaptive_smoothing.html#b-spline-basis",
    "title": "Adaptive smoothing in mgcv",
    "section": "B-spline basis",
    "text": "B-spline basis\nThe B-spline basis is an interesting and useful one. We setup the basis functions by recursion, so an order \\(m+1\\) basis function is to multiply some fixed thing (which we‚Äôll avoid writing-out here because I don‚Äôt think it adds much to the explanation) by the \\(m^\\text{th}\\) order basis function. The \\(-1^\\text{th}\\) order basis functions are just 1 inside a range and 0 outside.\nWe place the B-spline basis at knots (the knot is the centre of the basis function) and a basis function of order \\(m+1\\) (where \\(m=2\\) is a cubic spline, for boring definitional reasons) will be non-zero over the \\(m+3\\) adjacent knots. This, along with the regular placement of knots (more on that in a moment) means that B-splines are a local basis (that their effects are felt only in the vicinity of their knot). This makes them easy to interpret (compared to, say, thin plate splines). Figure¬†1 shows some cubic B-spline basis functions.\n\n# taken from Wood (2017)\nbspline &lt;- function(x,k,i,m=2){\n# evaluate ith b-spline basis function of order m at the values\n# in x, given knot locations in k\n  if (m==-1){ # base of recursion\n    res &lt;- as.numeric(x&lt;k[i+1]&x&gt;=k[i])\n  }else{\n    # construct from call to lower order basis\n    z0 &lt;- (x-k[i])/(k[i+m+1]-k[i])\n    z1 &lt;- (k[i+m+2]-x)/(k[i+m+2]-k[i+1])\n    res &lt;- z0*bspline(x,k,i,m-1)+ z1*bspline(x,k,i+1,m-1)\n  }\n  res\n}\n\n# knot locations\nnk &lt;- 16\nknts &lt;- seq(-0.5, 1.5, length.out=nk)\n# grid for evaluation\nxx &lt;- seq(0, 1, length.out=200)\n\nplot(c(0,1), c(0, 0.7), type=\"n\",\n     xlab=\"x\", ylab=\"b(x)\")\n\nfor(i in 1:nk){\n  lines(xx, bspline(xx, knts, i, 2), lty=2)\n}\n\n\n\n\n\n\n\nFigure¬†1: A B-spline basis.\n\n\n\n\n\nMore in-depth information on B-splines can be found in classic texts like DeBoor (1978). For more on their implementation in mgcv you can see Section 5.3.3 of SN Wood (2017) and Simon N Wood (2017)."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#p-spline-difference-penalties",
    "href": "articles/adaptive_smoothing.html#p-spline-difference-penalties",
    "title": "Adaptive smoothing in mgcv",
    "section": "P-spline difference penalties",
    "text": "P-spline difference penalties\nNow we could use the regular formulation to create a penalty for these P-splines: we can take derivatives and integrate over the range of the covariate. Instead, we can do something a bit faster and more efficient. This speed an efficiency comes at a cost of accuracy, but this often doesn‚Äôt matter much.\nRather than the derivative-based penalties we usually use for splines, we‚Äôre going to use a difference-based penalty. Where the differences are between the neighbouring \\(\\beta\\)s in the model.\nThe first order P-spline penalty is \\[\\begin{equation}\n        \\mathcal{P}_1 = \\sum_{k=1}^K (\\beta_{k+1}-\\beta_k)^2\n\\label{bs-firstorder-pen}\n\\end{equation}\\] where we have \\(K\\) basis functions (or knots if you think about the world that way). Note we still square the difference, just as we do in a derivative-based penalty.\nWe can re-write \\(\\eqref{bs-firstorder-pen}\\) in matrix-form by writing a matrix \\(\\mathbf{P}\\) such that: \\[\n\\mathbf{P} = \\pmatrix{-1 &  1 &  0 &  0 & 0 & \\cdots\\\\\n                       0 & -1 &  1 &  0 & 0 & \\cdots\\\\\n                       0 &  0 & -1 &  1 & 0 & \\cdots\\\\\n                         &    &    & \\ddots & \\ddots & & }\n\\] which just reflects the \\(\\beta_{k+1}-\\beta_k\\) term, taking the previous value away from the current one. Indeed, \\[\nP\\boldsymbol{\\beta} = \\pmatrix{\\beta_2-\\beta_1\\\\\n                               \\beta_3-\\beta_2\\\\\n                               \\beta_4-\\beta_3\\\\\n                               \\vdots},\n\\] and in order to get our penalty \\(\\mathcal{P}_1\\), we just need to take the product \\(\\mathbf{P}^\\intercal P\\) \\[\n\\mathcal{P}_1 = \\boldsymbol{\\beta}^\\intercal \\mathbf{P}^\\intercal \\mathbf{P}\\boldsymbol{\\beta} = \\sum_{k=1}^K (\\beta_{k+1}-\\beta_k)^2.\n\\]\nOur 2nd order penalty is \\[\\begin{equation}\n        \\mathcal{P}_2 = \\sum_{k=1}^K (\\beta_{k-1} -2\\beta_k +\\beta{i+1})^2\n\\label{bs-secondorder-pen}\n\\end{equation}\\] which gives us the following \\(\\mathbf{P}\\) matrix: \\[\n\\mathbf{P} = \\pmatrix{1 & -2 &  1 &  0 & 0 & \\cdots\\\\\n             0 & 1  & -2 &  1 & 0 & \\cdots\\\\\n             0 & 0  &  1 & -2 & 1 & \\cdots\\\\\n               &    &    &  \\ddots  & \\ddots &\\ddots  & }.\n\\]\nNow, these matrices are extremely fast to compute in R, because we just need to use the diff function on a diagonal matrix:\n\n(P1 &lt;- diff(diag(10), differences=1))\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   -1    1    0    0    0    0    0    0    0     0\n [2,]    0   -1    1    0    0    0    0    0    0     0\n [3,]    0    0   -1    1    0    0    0    0    0     0\n [4,]    0    0    0   -1    1    0    0    0    0     0\n [5,]    0    0    0    0   -1    1    0    0    0     0\n [6,]    0    0    0    0    0   -1    1    0    0     0\n [7,]    0    0    0    0    0    0   -1    1    0     0\n [8,]    0    0    0    0    0    0    0   -1    1     0\n [9,]    0    0    0    0    0    0    0    0   -1     1\n\n(P2 &lt;- diff(diag(10), differences=2))\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1   -2    1    0    0    0    0    0    0     0\n[2,]    0    1   -2    1    0    0    0    0    0     0\n[3,]    0    0    1   -2    1    0    0    0    0     0\n[4,]    0    0    0    1   -2    1    0    0    0     0\n[5,]    0    0    0    0    1   -2    1    0    0     0\n[6,]    0    0    0    0    0    1   -2    1    0     0\n[7,]    0    0    0    0    0    0    1   -2    1     0\n[8,]    0    0    0    0    0    0    0    1   -2     1\n\n\nNo differentiation, no integration. Lovely."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#more-options",
    "href": "articles/adaptive_smoothing.html#more-options",
    "title": "Adaptive smoothing in mgcv",
    "section": "More options",
    "text": "More options\nThere are a few things that we can change in the adaptive smoothing specification.\n\nBasis: we can use a fairly restricted set of basis functions can be used for the smoother (P-splines \"ps\", cyclic P-splines \"cp\", cyclic cubic splines \"cc\" or cubic splines \"cr\"), the penalty basis can only be a P-spline or cyclic P-spline. These are set via the xt=list(...) option to s(), specifically the $bs element, as a character vector (e.g., xt=list(bs=c(\"cp\", \"cp\")) for cyclic smoother and penalty).\nPenalty basis size: this is controlled by the m argument to s(). Note that this can‚Äôt be too big, as the model will get slow very quickly. It‚Äôs best to start small and increase slowly. Due to the local nature of B-splines, we are estimating the \\(\\gamma_j\\) parameters from a much reduced subset of data. From the help file ‚Äúsetting m=10 for a univariate smooth of 200 data is rather like estimating 10 smoothing parameters, each from a data series of length 20.‚Äù\n\nNote that adaptive smoothers can‚Äôt be used with gamm or as marginals in tensor product smooths. We can make 2-dimensional version of these smoothers (and fit that in mgcv) for cases where we have changing smoothness in space."
  },
  {
    "objectID": "articles/adaptive_smoothing.html#footnotes",
    "href": "articles/adaptive_smoothing.html#footnotes",
    "title": "Adaptive smoothing in mgcv",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough this report doesn‚Äôt appear to be findable, one can find a rather gruesome NATO conference proceedings from around the same time by the same authors which appears to demonstrate the same ideas here‚Ü©Ô∏é\nThe first element of the column vector \\(\\mathbf{B}\\boldsymbol{\\gamma}\\) is \\(\\mathbf{B}_{11}\\lambda_1 +\\mathbf{B}_{12}\\lambda_2 + \\ldots = \\sum_j \\lambda_j \\mathbf{B}_{1j}\\) and so on for the second row etc.‚Ü©Ô∏é\nThere is a more complicated way but I‚Äôll save that for another article.‚Ü©Ô∏é\nRead ‚Äútraumatic head injury‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "articles/prediction_intervals.html",
    "href": "articles/prediction_intervals.html",
    "title": "Prediction intervals",
    "section": "",
    "text": "We can make two kinds of predictions from our model [Faraway (2002); Section 3.5]:\nIn both cases the mean effect will be the same (so we can simply use the output from predict(model) but there are differences in how to calculate the uncertainty. In the latter case we need to include the additional variance from the error term in the model (usually denoted \\(\\epsilon\\)). We call this second case a prediction interval.\nThinking about the normal response case (ignoring link functions etc for now), if we can write down the linear predictor of our model as \\(\\boldsymbol{X}\\boldsymbol{\\beta}\\), we want \\(\\text{Var}(\\boldsymbol{X}\\boldsymbol{\\beta})\\) in case 1. above. For case 2. we have the prediction as \\(\\boldsymbol{X}\\boldsymbol{\\beta} + \\epsilon\\). \\(\\mathbf{E}(\\epsilon) = 0\\), so we ignore this for the mean effect, but we need to take \\(\\epsilon\\) into account.\nMore generally, we want to take into account uncertainty from the various hyperparameters of the response distribution, like scale, shape etc.\nIt‚Äôs simplest to show this via posterior simulation, so that‚Äôs what is used here."
  },
  {
    "objectID": "articles/prediction_intervals.html#example-1---normal-response",
    "href": "articles/prediction_intervals.html#example-1---normal-response",
    "title": "Prediction intervals",
    "section": "Example 1 - normal response",
    "text": "Example 1 - normal response\nFor the first example, we‚Äôll simulate some normally-distributed data using the gamSim function in mgcv, but only use one predictor, for simplicity.\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n\n## simulate some data...\nset.seed(8)\ndat &lt;- gamSim()\n\nGu & Wahba 4 term additive model\n\n# just want one predictor here to make y just f2 + noise\ndat$y &lt;- dat$f2 + rnorm(400, 0, 1)\n\nWe can then fit an extremely boring model to this data:\n\n## fit smooth model to x, y data...\nb &lt;- gam(y~s(x2,k=20), method=\"REML\", data=dat)\n\nNow to the central bit of the problem\n\n## simulate replicate beta vectors from posterior...\nn.rep &lt;- 10000\nbr &lt;- rmvn(n.rep, coef(b), vcov(b))\n\n## turn these into replicate smooths\nxp &lt;- seq(0, 1, length.out=200)\n\n# this is the \"Lp matrix\" which maps the coefficients to the predictors\n# you can think of it like the design matrix but for the predictions\nXp &lt;- predict(b, newdata=data.frame(x2=xp), type=\"lpmatrix\")\n\n# note that we should apply the link function here if we have one\n# this is now a matrix with n.rep replicate smooths over length(xp) locations\nfv &lt;- Xp%*%t(br)\n\n# now simulate from normal deviates with mean as in fv\n# and estimated scale...\nyr &lt;- matrix(rnorm(length(fv), fv, b$scale), nrow(fv), ncol(fv))\n\nnow make some plots\n\n# plot the replicates in yr\nplot(rep(xp, n.rep), yr, pch=\".\")\n# and the data we used to fit the model\npoints(dat$x2, dat$y, pch=19, cex=.5)\n\n# compute 95% prediction interval\n# since yr is a matrix where each row is a data location along x2 and\n# each column is a replicate we want the quantiles over the rows, summarizing\n# the replicates each time\nPI &lt;- apply(yr, 1, quantile, prob=c(.025,0.975))\n# the result has the 2.5% bound in the first row and the 97.5% bound\n# in the second\n\n# we can then plot those two bounds\nlines(xp, PI[1,], col=2, lwd=2)\nlines(xp, PI[2,], col=2, lwd=2)\n\n# and optionally add the confidence interval for comparison...\npred &lt;- predict(b, newdata=data.frame(x2=xp), se=TRUE)\nlines(xp, pred$fit, col=3, lwd=2)\nu.ci &lt;- pred$fit + 2*pred$se.fit\nl.ci &lt;- pred$fit - 2*pred$se.fit\nlines(xp, u.ci, col=3, lwd=2)\nlines(xp, l.ci, col=3, lwd=2)"
  }
]